-- This file and its contents are licensed under the Timescale License.
-- Please see the included NOTICE for copyright information and
-- LICENSE-TIMESCALE for a copy of the license.
\set ANALYZE 'EXPLAIN (analyze, costs off, timing off, summary off)'
-- test constraint exclusion with prepared statements and generic plans
CREATE TABLE i3719 (time timestamptz NOT NULL,data text);
SELECT table_name FROM create_hypertable('i3719', 'time');
 table_name 
 i3719
(1 row)

ALTER TABLE i3719 SET (timescaledb.compress);
WARNING:  there was some uncertainty picking the default segment by for the hypertable: You do not have any indexes on columns that can be used for segment_by and thus we are not using segment_by for compression. Please make sure you are not missing any indexes
NOTICE:  default segment by for hypertable "i3719" is set to ""
NOTICE:  default order by for hypertable "i3719" is set to ""time" DESC"
INSERT INTO i3719 VALUES('2021-01-01 00:00:00', 'chunk 1');
SELECT count(compress_chunk(c)) FROM show_chunks('i3719') c;
 count 
     1
(1 row)

INSERT INTO i3719 VALUES('2021-02-22 08:00:00', 'chunk 2');
SET plan_cache_mode TO force_generic_plan;
PREPARE p1(timestamptz) AS UPDATE i3719 SET data = 'x' WHERE time=$1;
PREPARE p2(timestamptz) AS DELETE FROM i3719 WHERE time=$1;
EXECUTE p1('2021-02-22T08:00:00+00');
EXECUTE p2('2021-02-22T08:00:00+00');
DEALLOCATE p1;
DEALLOCATE p2;
DROP TABLE i3719;
-- github issue 4778
CREATE TABLE metric_5m (
    time TIMESTAMPTZ NOT NULL,
    value DOUBLE PRECISION NOT NULL,
    series_id BIGINT NOT NULL
);
SELECT table_name FROM create_hypertable(
                            'metric_5m'::regclass,
                            'time'::name, chunk_time_interval=>interval '5m',
                            create_default_indexes=> false);
 table_name 
 metric_5m
(1 row)

-- enable compression
ALTER TABLE metric_5m SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'series_id',
    timescaledb.compress_orderby = 'time, value'
);
SET work_mem TO '64kB';
SELECT '2022-10-10 14:33:44.1234+05:30' as start_date \gset
-- populate hypertable
INSERT INTO metric_5m (time, series_id, value)
    SELECT t, s,1 from generate_series(:'start_date'::timestamptz, :'start_date'::timestamptz + interval '1 day', '10s') t cross join generate_series(1,10, 1) s;
-- manually compress all chunks
SELECT count(compress_chunk(c)) FROM show_chunks('metric_5m') c;
 count 
   289
(1 row)

-- populate into compressed hypertable, this should not crash
INSERT INTO metric_5m (time, series_id, value)
    SELECT t, s,1 from generate_series(:'start_date'::timestamptz, :'start_date'::timestamptz + interval '1 day', '10s') t cross join generate_series(1,10, 1) s;
-- clean up
RESET work_mem;
DROP TABLE metric_5m;
-- github issue 5134
CREATE TABLE mytab (time TIMESTAMPTZ NOT NULL, a INT, b INT, c INT);
SELECT table_name FROM create_hypertable('mytab', 'time', chunk_time_interval => interval '1 day');
 table_name 
 mytab
(1 row)

INSERT INTO mytab
    SELECT time,
        CASE WHEN (:'start_date'::timestamptz - time < interval '1 days') THEN 1
             WHEN (:'start_date'::timestamptz - time < interval '2 days') THEN 2
             WHEN (:'start_date'::timestamptz - time < interval '3 days') THEN 3 ELSE 4 END as a
    from generate_series(:'start_date'::timestamptz - interval '3 days', :'start_date'::timestamptz, interval '5 sec') as g1(time);
-- enable compression
ALTER TABLE mytab SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'a, c'
);
NOTICE:  default order by for hypertable "mytab" is set to ""time" DESC"
-- get first chunk name
SELECT chunk_schema || '.' || chunk_name as "chunk_table"
       FROM timescaledb_information.chunks
       WHERE hypertable_name = 'mytab' ORDER BY range_start limit 1 \gset
-- compress only the first chunk
SELECT count(compress_chunk(:'chunk_table'));
 count 
     1
(1 row)

-- insert a row into first compressed chunk
INSERT INTO mytab SELECT '2022-10-07 05:30:10+05:30'::timestamp with time zone, 3, 3;
-- should not crash
EXPLAIN (costs off) SELECT * FROM :chunk_table;
QUERY PLAN
 Append
   ->  Custom Scan (DecompressChunk) on _hyper_X_X_chunk
         ->  Seq Scan on compress_hyper_X_X_chunk
   ->  Seq Scan on _hyper_X_X_chunk
(4 rows)

DROP TABLE mytab CASCADE;
-- test varchar segmentby
CREATE TABLE comp_seg_varchar (
  time timestamptz NOT NULL,
  source_id varchar(64) NOT NULL,
  label varchar NOT NULL,
  data jsonb
);
SELECT table_name FROM create_hypertable('comp_seg_varchar', 'time');
WARNING:  column type "character varying" used for "source_id" does not follow best practices
WARNING:  column type "character varying" used for "label" does not follow best practices
    table_name    
 comp_seg_varchar
(1 row)

CREATE UNIQUE INDEX ON comp_seg_varchar(source_id, label, "time" DESC);
ALTER TABLE comp_seg_varchar SET(timescaledb.compress, timescaledb.compress_segmentby = 'source_id, label', timescaledb.compress_orderby = 'time');
INSERT INTO comp_seg_varchar
SELECT time, source_id, label, '{}' AS data
FROM
generate_series('1990-01-01'::timestamptz, '1990-01-10'::timestamptz, INTERVAL '1 day') AS g1(time),
generate_series(1, 3, 1 ) AS g2(source_id),
generate_series(1, 3, 1 ) AS g3(label);
SELECT count(compress_chunk(c)) FROM show_chunks('comp_seg_varchar') c;
 count 
     2
(1 row)

-- all tuples should come from compressed chunks
EXPLAIN (analyze,costs off, timing off, summary off) SELECT * FROM comp_seg_varchar;
QUERY PLAN
 Append (actual rows=90 loops=1)
   ->  Custom Scan (DecompressChunk) on _hyper_X_X_chunk (actual rows=27 loops=1)
         ->  Seq Scan on compress_hyper_X_X_chunk (actual rows=9 loops=1)
   ->  Custom Scan (DecompressChunk) on _hyper_X_X_chunk (actual rows=63 loops=1)
         ->  Seq Scan on compress_hyper_X_X_chunk (actual rows=9 loops=1)
(5 rows)

INSERT INTO comp_seg_varchar(time, source_id, label, data) VALUES ('1990-01-02 00:00:00+00', 'test', 'test', '{}'::jsonb)
ON CONFLICT (source_id, label, time) DO UPDATE SET data = '{"update": true}';
-- no tuples should be moved into uncompressed
EXPLAIN (analyze,costs off, timing off, summary off) SELECT * FROM comp_seg_varchar;
QUERY PLAN
 Append (actual rows=91 loops=1)
   ->  Custom Scan (DecompressChunk) on _hyper_X_X_chunk (actual rows=27 loops=1)
         ->  Seq Scan on compress_hyper_X_X_chunk (actual rows=9 loops=1)
   ->  Seq Scan on _hyper_X_X_chunk (actual rows=1 loops=1)
   ->  Custom Scan (DecompressChunk) on _hyper_X_X_chunk (actual rows=63 loops=1)
         ->  Seq Scan on compress_hyper_X_X_chunk (actual rows=9 loops=1)
(6 rows)

INSERT INTO comp_seg_varchar(time, source_id, label, data) VALUES ('1990-01-02 00:00:00+00', '1', '2', '{}'::jsonb)
ON CONFLICT (source_id, label, time) DO UPDATE SET data = '{"update": true}';
-- 1 batch should be moved into uncompressed
EXPLAIN (analyze,costs off, timing off, summary off) SELECT * FROM comp_seg_varchar;
QUERY PLAN
 Append (actual rows=92 loops=1)
   ->  Custom Scan (DecompressChunk) on _hyper_X_X_chunk (actual rows=27 loops=1)
         ->  Seq Scan on compress_hyper_X_X_chunk (actual rows=9 loops=1)
   ->  Seq Scan on _hyper_X_X_chunk (actual rows=2 loops=1)
   ->  Custom Scan (DecompressChunk) on _hyper_X_X_chunk (actual rows=63 loops=1)
         ->  Seq Scan on compress_hyper_X_X_chunk (actual rows=9 loops=1)
(6 rows)

DROP TABLE comp_seg_varchar;
-- test row locks for compressed tuples are blocked
CREATE TABLE row_locks(time timestamptz NOT NULL);
SELECT table_name FROM create_hypertable('row_locks', 'time');
 table_name 
 row_locks
(1 row)

ALTER TABLE row_locks SET (timescaledb.compress);
WARNING:  there was some uncertainty picking the default segment by for the hypertable: You do not have any indexes on columns that can be used for segment_by and thus we are not using segment_by for compression. Please make sure you are not missing any indexes
NOTICE:  default segment by for hypertable "row_locks" is set to ""
NOTICE:  default order by for hypertable "row_locks" is set to ""time" DESC"
INSERT INTO row_locks VALUES('2021-01-01 00:00:00');
SELECT count(compress_chunk(c)) FROM show_chunks('row_locks') c;
 count 
     1
(1 row)

-- should succeed cause no compressed tuples are returned
SELECT FROM row_locks WHERE time < '2021-01-01 00:00:00' FOR UPDATE;
(0 rows)

-- should be blocked
\set ON_ERROR_STOP 0
SELECT FROM row_locks FOR UPDATE;
ERROR:  locking compressed tuples is not supported
SELECT FROM row_locks FOR NO KEY UPDATE;
ERROR:  locking compressed tuples is not supported
SELECT FROM row_locks FOR SHARE;
ERROR:  locking compressed tuples is not supported
SELECT FROM row_locks FOR KEY SHARE;
ERROR:  locking compressed tuples is not supported
\set ON_ERROR_STOP 1
DROP TABLE row_locks;
CREATE TABLE lazy_decompress(time timestamptz not null, device text, value float, primary key (device,time));
SELECT table_name FROM create_hypertable('lazy_decompress', 'time');
   table_name    
 lazy_decompress
(1 row)

ALTER TABLE lazy_decompress SET (timescaledb.compress, timescaledb.compress_segmentby = 'device');
NOTICE:  default order by for hypertable "lazy_decompress" is set to ""time" DESC"
INSERT INTO lazy_decompress SELECT '2024-01-01'::timestamptz + format('%s',i)::interval, 'd1', i FROM generate_series(1,6000) g(i);
SELECT count(compress_chunk(c)) FROM show_chunks('lazy_decompress') c;
 count 
     1
(1 row)

-- no decompression cause no match in batch
BEGIN; :ANALYZE INSERT INTO lazy_decompress SELECT '2024-01-01 0:00:00.5','d1',random() ON CONFLICT DO NOTHING; ROLLBACK;
QUERY PLAN
 Custom Scan (HypertableModify) (actual rows=0 loops=1)
   ->  Insert on lazy_decompress (actual rows=0 loops=1)
         Conflict Resolution: NOTHING
         Tuples Inserted: 1
         Conflicting Tuples: 0
         ->  Custom Scan (ChunkDispatch) (actual rows=1 loops=1)
               ->  Subquery Scan on "*SELECT*" (actual rows=1 loops=1)
                     ->  Result (actual rows=1 loops=1)
(8 rows)

BEGIN; :ANALYZE INSERT INTO lazy_decompress SELECT '2024-01-01 0:00:00.5','d1',random() ON CONFLICT(time,device) DO UPDATE SET value=EXCLUDED.value; ROLLBACK;
QUERY PLAN
 Custom Scan (HypertableModify) (actual rows=0 loops=1)
   ->  Insert on lazy_decompress (actual rows=0 loops=1)
         Conflict Resolution: UPDATE
         Conflict Arbiter Indexes: lazy_decompress_pkey
         Tuples Inserted: 1
         Conflicting Tuples: 0
         ->  Custom Scan (ChunkDispatch) (actual rows=1 loops=1)
               ->  Subquery Scan on "*SELECT*" (actual rows=1 loops=1)
                     ->  Result (actual rows=1 loops=1)
(9 rows)

-- should decompress 1 batch cause there is match
BEGIN; :ANALYZE INSERT INTO lazy_decompress SELECT '2024-01-01 0:00:01','d1',random() ON CONFLICT DO NOTHING; ROLLBACK;
QUERY PLAN
 Custom Scan (HypertableModify) (actual rows=0 loops=1)
   Batches decompressed: 1
   Tuples decompressed: 1000
   ->  Insert on lazy_decompress (actual rows=0 loops=1)
         Conflict Resolution: NOTHING
         Tuples Inserted: 0
         Conflicting Tuples: 1
         ->  Custom Scan (ChunkDispatch) (actual rows=1 loops=1)
               ->  Subquery Scan on "*SELECT*" (actual rows=1 loops=1)
                     ->  Result (actual rows=1 loops=1)
(10 rows)

