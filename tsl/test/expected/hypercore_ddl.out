-- This file and its contents are licensed under the Timescale License.
-- Please see the included NOTICE for copyright information and
-- LICENSE-TIMESCALE for a copy of the license.
\set ECHO errors
-- Disable incremental sort to make tests stable
set enable_incremental_sort = false;
select setseed(1);
 setseed 
---------
 
(1 row)

create table readings(
       time timestamptz not null unique,
       location int not null,
       device int not null,
       temp numeric(4,1),
       humidity float,
       jdata jsonb
);
select create_hypertable('readings', by_range('time', '1d'::interval));
 create_hypertable 
-------------------
 (1,t)
(1 row)

alter table readings
      set (timescaledb.compress_orderby = 'time',
	   timescaledb.compress_segmentby = 'device');
insert into readings (time, location, device, temp, humidity, jdata)
select t, ceil(random()*10), ceil(random()*30), random()*40, random()*100, '{"a":1,"b":2}'::jsonb
from generate_series('2022-06-01'::timestamptz, '2022-06-04'::timestamptz, '5m') t;
select compress_chunk(show_chunks('readings'), hypercore_use_access_method => true);
             compress_chunk             
----------------------------------------
 _timescaledb_internal._hyper_1_1_chunk
 _timescaledb_internal._hyper_1_2_chunk
 _timescaledb_internal._hyper_1_3_chunk
 _timescaledb_internal._hyper_1_4_chunk
(4 rows)

select chunk, amname from chunk_info where hypertable = 'readings'::regclass;
                 chunk                  |  amname   
----------------------------------------+-----------
 _timescaledb_internal._hyper_1_1_chunk | hypercore
 _timescaledb_internal._hyper_1_2_chunk | hypercore
 _timescaledb_internal._hyper_1_3_chunk | hypercore
 _timescaledb_internal._hyper_1_4_chunk | hypercore
(4 rows)

-- Pick a chunk to play with that is not the first chunk. This is
-- mostly a precaution to make sure that there is no bias towards the
-- first chunk and we could just as well pick the first chunk.
select chunk from show_chunks('readings') x(chunk) limit 1 offset 3 \gset
----------------------------------------------------------------
-- Test ALTER TABLE .... ALTER COLUMN commands
-- This should fail since "location" is NOT NULL
\set ON_ERROR_STOP 0
insert into readings(time,device,temp,humidity,jdata)
values ('2024-01-01 00:00:10', 1, 99.0, 99.0, '{"magic": "yes"}'::jsonb);
ERROR:  null value in column "location" of relation "_hyper_1_9_chunk" violates not-null constraint
\set ON_ERROR_STOP 1
-- Test altering column definitions to drop NOT NULL and check that it
-- propagates to the chunks. We just pick one chunk here and check
-- that the setting propagates.
alter table readings alter column location drop not null;
\d readings
                       Table "public.readings"
  Column  |           Type           | Collation | Nullable | Default 
----------+--------------------------+-----------+----------+---------
 time     | timestamp with time zone |           | not null | 
 location | integer                  |           |          | 
 device   | integer                  |           | not null | 
 temp     | numeric(4,1)             |           |          | 
 humidity | double precision         |           |          | 
 jdata    | jsonb                    |           |          | 
Indexes:
    "readings_time_key" UNIQUE CONSTRAINT, btree ("time")
Triggers:
    ts_insert_blocker BEFORE INSERT ON readings FOR EACH ROW EXECUTE FUNCTION _timescaledb_functions.insert_blocker()
Number of child tables: 4 (Use \d+ to list them.)

\d :chunk
            Table "_timescaledb_internal._hyper_1_4_chunk"
  Column  |           Type           | Collation | Nullable | Default 
----------+--------------------------+-----------+----------+---------
 time     | timestamp with time zone |           | not null | 
 location | integer                  |           |          | 
 device   | integer                  |           | not null | 
 temp     | numeric(4,1)             |           |          | 
 humidity | double precision         |           |          | 
 jdata    | jsonb                    |           |          | 
Indexes:
    "4_4_readings_time_key" UNIQUE CONSTRAINT, btree ("time")
Check constraints:
    "constraint_4" CHECK ("time" >= 'Fri Jun 03 17:00:00 2022 PDT'::timestamp with time zone AND "time" < 'Sat Jun 04 17:00:00 2022 PDT'::timestamp with time zone)
Inherits: readings

-- This should now work since we allow NULL values
insert into readings(time,device,temp,humidity,jdata)
values ('2024-01-01 00:00:10', 1, 99.0, 99.0, '{"magic": "yes"}'::jsonb);
select count(*) from readings where location is null;
 count 
-------
     1
(1 row)

select compress_chunk(show_chunks('readings'), hypercore_use_access_method => true);
NOTICE:  chunk "_hyper_1_1_chunk" is already converted to columnstore
NOTICE:  chunk "_hyper_1_2_chunk" is already converted to columnstore
NOTICE:  chunk "_hyper_1_3_chunk" is already converted to columnstore
NOTICE:  chunk "_hyper_1_4_chunk" is already converted to columnstore
             compress_chunk              
-----------------------------------------
 _timescaledb_internal._hyper_1_1_chunk
 _timescaledb_internal._hyper_1_2_chunk
 _timescaledb_internal._hyper_1_3_chunk
 _timescaledb_internal._hyper_1_4_chunk
 _timescaledb_internal._hyper_1_10_chunk
(5 rows)

select count(*) from readings where location is null;
 count 
-------
     1
(1 row)

-- We insert another row with nulls, that will end up in the
-- non-compressed region.
insert into readings(time,device,temp,humidity,jdata)
values ('2024-01-02 00:00:10', 1, 66.0, 66.0, '{"magic": "more"}'::jsonb);
-- We should not be able to set the not null before we have removed
-- the null rows in the table. This works for hypercore-compressed
-- chunks but not for heap-compressed chunks.
\set ON_ERROR_STOP 0
alter table readings alter column location set not null;
ERROR:  column "location" of relation "_hyper_1_10_chunk" contains null values
\set ON_ERROR_STOP 1
delete from readings where location is null;
-- Compress the data to make sure that we are not working on
-- non-compressed data.
select compress_chunk(show_chunks('readings'), hypercore_use_access_method => true);
             compress_chunk              
-----------------------------------------
 _timescaledb_internal._hyper_1_1_chunk
 _timescaledb_internal._hyper_1_2_chunk
 _timescaledb_internal._hyper_1_3_chunk
 _timescaledb_internal._hyper_1_4_chunk
 _timescaledb_internal._hyper_1_10_chunk
 _timescaledb_internal._hyper_1_12_chunk
(6 rows)

select count(*) from readings where location is null;
 count 
-------
     0
(1 row)

alter table readings alter column location set not null;
\d readings
                       Table "public.readings"
  Column  |           Type           | Collation | Nullable | Default 
----------+--------------------------+-----------+----------+---------
 time     | timestamp with time zone |           | not null | 
 location | integer                  |           | not null | 
 device   | integer                  |           | not null | 
 temp     | numeric(4,1)             |           |          | 
 humidity | double precision         |           |          | 
 jdata    | jsonb                    |           |          | 
Indexes:
    "readings_time_key" UNIQUE CONSTRAINT, btree ("time")
Triggers:
    ts_insert_blocker BEFORE INSERT ON readings FOR EACH ROW EXECUTE FUNCTION _timescaledb_functions.insert_blocker()
Number of child tables: 6 (Use \d+ to list them.)

\d :chunk
            Table "_timescaledb_internal._hyper_1_4_chunk"
  Column  |           Type           | Collation | Nullable | Default 
----------+--------------------------+-----------+----------+---------
 time     | timestamp with time zone |           | not null | 
 location | integer                  |           | not null | 
 device   | integer                  |           | not null | 
 temp     | numeric(4,1)             |           |          | 
 humidity | double precision         |           |          | 
 jdata    | jsonb                    |           |          | 
Indexes:
    "4_4_readings_time_key" UNIQUE CONSTRAINT, btree ("time")
Check constraints:
    "constraint_4" CHECK ("time" >= 'Fri Jun 03 17:00:00 2022 PDT'::timestamp with time zone AND "time" < 'Sat Jun 04 17:00:00 2022 PDT'::timestamp with time zone)
Inherits: readings

select count(*) from readings where location is null;
 count 
-------
     0
(1 row)

----------------------------------------------------------------
-- TRUNCATE test
-- We keep the truncate test last in the file to avoid having to
-- re-populate it.
-- Insert some extra data to get some non-compressed data as
-- well. This checks that truncate will deal with with write-store
-- (WS) and read-store (RS)
insert into readings (time, location, device, temp, humidity, jdata)
select t, ceil(random()*10), ceil(random()*30), random()*40, random()*100, '{"a":1,"b":2}'::jsonb
from generate_series('2022-06-01 00:01:00'::timestamptz, '2022-06-04'::timestamptz, '5m') t;
-- Check that the number of bytes in the table before and after the
-- truncate.
--
-- Note that a table with a toastable attribute will always have a
-- toast table assigned, so pg_table_size() shows one page allocated
-- since this includes the toast table.
select pg_table_size(chunk) as chunk_size,
       pg_table_size(compressed_chunk) as compressed_chunk_size
  from chunk_info
 where chunk = :'chunk'::regclass;
 chunk_size | compressed_chunk_size 
------------+-----------------------
      49152 |                 73728
(1 row)

truncate :chunk;
select pg_table_size(chunk) as chunk_size,
       pg_table_size(compressed_chunk) as compressed_chunk_size
  from chunk_info
 where chunk = :'chunk'::regclass;
 chunk_size | compressed_chunk_size 
------------+-----------------------
          0 |                  8192
(1 row)

-- We test TRUNCATE on a hypertable as well, but truncating a
-- hypertable is done by deleting all chunks, not by truncating each
-- chunk.
select (select count(*) from readings) tuples,
       (select count(*) from show_chunks('readings')) chunks;
 tuples | chunks 
--------+--------
   1560 |      6
(1 row)

truncate readings;
select (select count(*) from readings) tuples,
       (select count(*) from show_chunks('readings')) chunks;
 tuples | chunks 
--------+--------
      0 |      0
(1 row)

