-- This file and its contents are licensed under the Timescale License.
-- Please see the included NOTICE for copyright information and
-- LICENSE-TIMESCALE for a copy of the license.
\ir include/setup_hyperstore.sql
-- This file and its contents are licensed under the Timescale License.
-- Please see the included NOTICE for copyright information and
-- LICENSE-TIMESCALE for a copy of the license.
\set hypertable readings
-- Alternative function to compress_chunk that uses the table access
-- method to compress a chunk.
create function twist_chunk(chunk regclass) returns regclass language plpgsql
as $$
begin
    execute format('alter table %s set access method hyperstore', chunk);
    return chunk;
end
$$;
create function untwist_chunk(chunk regclass) returns regclass language plpgsql
as $$
begin
    execute format('alter table %s set access method heap', chunk);
    return chunk;
end
$$;
create table :hypertable(
       metric_id serial,
       created_at timestamptz not null unique,
       location_id int,		--segmentby attribute with index
       owner_id int,		--segmentby attribute without index
       device_id int,		--non-segmentby attribute
       temp float,
       humidity float
);
create index hypertable_location_id_idx on :hypertable (location_id);
create index hypertable_device_id_idx on :hypertable (device_id);
select create_hypertable(:'hypertable', by_range('created_at'));
 create_hypertable 
-------------------
 (1,t)
(1 row)

-- Disable incremental sort to make tests stable
set enable_incremental_sort = false;
select setseed(1);
 setseed 
---------
 
(1 row)

-- Insert rows into the tables.
--
-- The timestamps for the original rows will have timestamps every 10
-- seconds. Any other timestamps are inserted as part of the test.
insert into :hypertable (created_at, location_id, owner_id, device_id, temp, humidity)
select t, ceil(random()*10), ceil(random()*30), ceil(random() * 5), random()*40, random()*100
from generate_series('2022-06-01'::timestamptz, '2022-07-01', '10s') t;
alter table :hypertable set (
	  timescaledb.compress,
	  timescaledb.compress_orderby = 'created_at',
	  timescaledb.compress_segmentby = 'location_id, owner_id'
);
-- Get some test chunks as global variables (first and last chunk here)
select format('%I.%I', chunk_schema, chunk_name)::regclass as chunk1
  from timescaledb_information.chunks
 where format('%I.%I', hypertable_schema, hypertable_name)::regclass = :'hypertable'::regclass
 order by chunk1 asc
 limit 1 \gset
select format('%I.%I', chunk_schema, chunk_name)::regclass as chunk2
  from timescaledb_information.chunks
 where format('%I.%I', hypertable_schema, hypertable_name)::regclass = :'hypertable'::regclass
 order by chunk2 asc
 limit 1 offset 1 \gset
-- We need to drop the index to trigger parallel plans. Otherwise they
-- will use the index.
drop index hypertable_device_id_idx;
-- Show parallel plan and count on uncompressed (non-hyperstore)
-- hypertable
set max_parallel_workers_per_gather=2;
explain (costs off)
select device_id, count(*) from :hypertable where device_id=1 group by device_id;
                          QUERY PLAN                           
---------------------------------------------------------------
 Finalize GroupAggregate
   ->  Gather
         Workers Planned: 2
         ->  Parallel Append
               ->  Partial GroupAggregate
                     ->  Parallel Seq Scan on _hyper_1_2_chunk
                           Filter: (device_id = 1)
               ->  Partial GroupAggregate
                     ->  Parallel Seq Scan on _hyper_1_3_chunk
                           Filter: (device_id = 1)
               ->  Partial GroupAggregate
                     ->  Parallel Seq Scan on _hyper_1_4_chunk
                           Filter: (device_id = 1)
               ->  Partial GroupAggregate
                     ->  Parallel Seq Scan on _hyper_1_5_chunk
                           Filter: (device_id = 1)
               ->  Partial GroupAggregate
                     ->  Parallel Seq Scan on _hyper_1_6_chunk
                           Filter: (device_id = 1)
               ->  Partial GroupAggregate
                     ->  Parallel Seq Scan on _hyper_1_1_chunk
                           Filter: (device_id = 1)
(22 rows)

select device_id, count(*) from :hypertable where device_id=1 group by device_id;
 device_id | count 
-----------+-------
         1 | 52277
(1 row)

-- Save counts collected over entire hypertable
select device_id, count(*) into orig from :hypertable group by device_id;
-- Save counts over a single chunk
select device_id, count(*) into orig_chunk from :chunk1 group by device_id;
-----------------------
-- Enable hyperstore --
-----------------------
select twist_chunk(show_chunks(:'hypertable'));
              twist_chunk               
----------------------------------------
 _timescaledb_internal._hyper_1_1_chunk
 _timescaledb_internal._hyper_1_2_chunk
 _timescaledb_internal._hyper_1_3_chunk
 _timescaledb_internal._hyper_1_4_chunk
 _timescaledb_internal._hyper_1_5_chunk
 _timescaledb_internal._hyper_1_6_chunk
(6 rows)

-- Show count without parallel plan and without ColumnarScan
set timescaledb.enable_columnarscan=false;
set max_parallel_workers_per_gather=0;
explain (costs off)
select device_id, count(*) from :hypertable where device_id=1 group by device_id;
                   QUERY PLAN                   
------------------------------------------------
 Finalize GroupAggregate
   ->  Append
         ->  Partial GroupAggregate
               ->  Seq Scan on _hyper_1_1_chunk
                     Filter: (device_id = 1)
         ->  Partial GroupAggregate
               ->  Seq Scan on _hyper_1_2_chunk
                     Filter: (device_id = 1)
         ->  Partial GroupAggregate
               ->  Seq Scan on _hyper_1_3_chunk
                     Filter: (device_id = 1)
         ->  Partial GroupAggregate
               ->  Seq Scan on _hyper_1_4_chunk
                     Filter: (device_id = 1)
         ->  Partial GroupAggregate
               ->  Seq Scan on _hyper_1_5_chunk
                     Filter: (device_id = 1)
         ->  Partial GroupAggregate
               ->  Seq Scan on _hyper_1_6_chunk
                     Filter: (device_id = 1)
(20 rows)

select device_id, count(*) from :hypertable where device_id=1 group by device_id;
 device_id | count 
-----------+-------
         1 | 52277
(1 row)

-- Enable parallel on SeqScan and check for same result
set max_parallel_workers_per_gather=2;
explain (costs off)
select device_id, count(*) from :hypertable where device_id=1 group by device_id;
                          QUERY PLAN                           
---------------------------------------------------------------
 Finalize GroupAggregate
   ->  Gather
         Workers Planned: 2
         ->  Parallel Append
               ->  Partial GroupAggregate
                     ->  Parallel Seq Scan on _hyper_1_1_chunk
                           Filter: (device_id = 1)
               ->  Partial GroupAggregate
                     ->  Parallel Seq Scan on _hyper_1_6_chunk
                           Filter: (device_id = 1)
               ->  Partial GroupAggregate
                     ->  Parallel Seq Scan on _hyper_1_2_chunk
                           Filter: (device_id = 1)
               ->  Partial GroupAggregate
                     ->  Parallel Seq Scan on _hyper_1_3_chunk
                           Filter: (device_id = 1)
               ->  Partial GroupAggregate
                     ->  Parallel Seq Scan on _hyper_1_4_chunk
                           Filter: (device_id = 1)
               ->  Partial GroupAggregate
                     ->  Parallel Seq Scan on _hyper_1_5_chunk
                           Filter: (device_id = 1)
(22 rows)

select device_id, count(*) from :hypertable where device_id=1 group by device_id;
 device_id | count 
-----------+-------
         1 | 52277
(1 row)

-- Enable ColumnarScan and check for same result
set timescaledb.enable_columnarscan=true;
explain (costs off)
select device_id, count(*) from :hypertable where device_id=1 group by device_id;
                                   QUERY PLAN                                    
---------------------------------------------------------------------------------
 Finalize GroupAggregate
   ->  Gather
         Workers Planned: 2
         ->  Parallel Append
               ->  Partial GroupAggregate
                     ->  Parallel Custom Scan (ColumnarScan) on _hyper_1_1_chunk
                           Vectorized Filter: (device_id = 1)
               ->  Partial GroupAggregate
                     ->  Parallel Custom Scan (ColumnarScan) on _hyper_1_6_chunk
                           Vectorized Filter: (device_id = 1)
               ->  Partial GroupAggregate
                     ->  Parallel Custom Scan (ColumnarScan) on _hyper_1_2_chunk
                           Vectorized Filter: (device_id = 1)
               ->  Partial GroupAggregate
                     ->  Parallel Custom Scan (ColumnarScan) on _hyper_1_3_chunk
                           Vectorized Filter: (device_id = 1)
               ->  Partial GroupAggregate
                     ->  Parallel Custom Scan (ColumnarScan) on _hyper_1_4_chunk
                           Vectorized Filter: (device_id = 1)
               ->  Partial GroupAggregate
                     ->  Parallel Custom Scan (ColumnarScan) on _hyper_1_5_chunk
                           Vectorized Filter: (device_id = 1)
(22 rows)

select device_id, count(*) from :hypertable where device_id=1 group by device_id;
 device_id | count 
-----------+-------
         1 | 52277
(1 row)

-- Parallel plan with hyperstore on single chunk
explain (costs off)
select device_id, count(*) from :chunk1 where device_id=1 group by device_id;
                      QUERY PLAN                      
------------------------------------------------------
 GroupAggregate
   ->  Custom Scan (ColumnarScan) on _hyper_1_1_chunk
         Vectorized Filter: (device_id = 1)
(3 rows)

select device_id, count(*) from :chunk1 where device_id=1 group by device_id;
 device_id | count 
-----------+-------
         1 |  1249
(1 row)

-- Compare hyperstore per-location counts with original counts without
-- hyperstore
select device_id, count(*) into comp from :hypertable group by device_id;
select * from orig join comp using (device_id) where orig.count != comp.count;
 device_id | count | count 
-----------+-------+-------
(0 rows)

-- Compare counts on single chunk
select device_id, count(*) into comp_chunk from :chunk1 group by device_id;
select * from orig_chunk join comp_chunk using (device_id) where orig_chunk.count != comp_chunk.count;
 device_id | count | count 
-----------+-------+-------
(0 rows)

