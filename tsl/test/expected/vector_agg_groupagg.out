-- This file and its contents are licensed under the Timescale License.
-- Please see the included NOTICE for copyright information and
-- LICENSE-TIMESCALE for a copy of the license.
-- Check that the vectorized aggregation works properly in the GroupAggregate
-- mode.
\pset null ¤
set max_parallel_workers_per_gather = 0;
set enable_hashagg to off;
set timescaledb.enable_vectorized_aggregation to off;
set timescaledb.debug_require_vector_agg to 'forbid';
create table groupagg(t int, s text, value int);
select create_hypertable('groupagg', 't', chunk_time_interval => 10000);
NOTICE:  adding not-null constraint to column "t"
   create_hypertable   
-----------------------
 (1,public,groupagg,t)
(1 row)

insert into groupagg
select
    xfast * 100 + xslow,
    case when xfast = 13 then null else xfast end,
    xfast * 7 + xslow * 3
from generate_series(10, 99) xfast,
    generate_series(1, 1000) xslow
;
alter table groupagg set (timescaledb.compress, timescaledb.compress_segmentby = '',
    timescaledb.compress_orderby = 's');
select count(compress_chunk(x)) from show_chunks('groupagg') x;
 count 
-------
     2
(1 row)

vacuum analyze groupagg;
select s, sum(value) from groupagg group by s order by s limit 10;
 s  |   sum   
----+---------
 10 | 1571500
 11 | 1578500
 12 | 1585500
 14 | 1599500
 15 | 1606500
 16 | 1613500
 17 | 1620500
 18 | 1627500
 19 | 1634500
 20 | 1641500
(10 rows)

-- The hash grouping policies do not support the GroupAggregate mode in the
-- reverse order.
select s, sum(value) from groupagg group by s order by s desc limit 10;
 s  |   sum   
----+---------
 ¤  | 1592500
 99 | 2194500
 98 | 2187500
 97 | 2180500
 96 | 2173500
 95 | 2166500
 94 | 2159500
 93 | 2152500
 92 | 2145500
 91 | 2138500
(10 rows)

select count(decompress_chunk(x)) from show_chunks('groupagg') x;
 count 
-------
     2
(1 row)

alter table groupagg set (timescaledb.compress, timescaledb.compress_segmentby = '',
    timescaledb.compress_orderby = 's nulls first');
select count(compress_chunk(x)) from show_chunks('groupagg') x;
 count 
-------
     2
(1 row)

vacuum analyze groupagg;
select s , sum(value) from groupagg group by s  order by s  nulls first limit 10;
 s  |   sum   
----+---------
 ¤  | 1592500
 10 | 1571500
 11 | 1578500
 12 | 1585500
 14 | 1599500
 15 | 1606500
 16 | 1613500
 17 | 1620500
 18 | 1627500
 19 | 1634500
(10 rows)

-- More tests for dictionary encoding.
create table text_table(ts int);
select create_hypertable('text_table', 'ts', chunk_time_interval => 3);
NOTICE:  adding not-null constraint to column "ts"
    create_hypertable    
-------------------------
 (3,public,text_table,t)
(1 row)

alter table text_table set (timescaledb.compress);
WARNING:  there was some uncertainty picking the default segment by for the hypertable: You do not have any indexes on columns that can be used for segment_by and thus we are not using segment_by for compression. Please make sure you are not missing any indexes
NOTICE:  default segment by for hypertable "text_table" is set to ""
NOTICE:  default order by for hypertable "text_table" is set to "ts DESC"
insert into text_table select 0 /*, default */ from generate_series(1, 1000) x;
select count(compress_chunk(x)) from show_chunks('text_table') x;
 count 
-------
     1
(1 row)

alter table text_table add column a text default 'default';
alter table text_table set (timescaledb.compress,
    timescaledb.compress_segmentby = '', timescaledb.compress_orderby = 'a');
insert into text_table select 1, '' from generate_series(1, 1000) x;
insert into text_table select 2, 'same' from generate_series(1, 1000) x;
insert into text_table select 3, 'different' || x from generate_series(1, 1000) x;
insert into text_table select 4, case when x % 2 = 0 then null else 'same-with-nulls' end from generate_series(1, 1000) x;
insert into text_table select 5, case when x % 2 = 0 then null else 'different-with-nulls' || x end from generate_series(1, 1000) x;
select count(compress_chunk(x)) from show_chunks('text_table') x;
 count 
-------
     2
(1 row)

vacuum analyze text_table;
explain (verbose, costs off, analyze, timing off, summary off)
select a, count(*) from text_table group by a order by a limit 10;
                                                                                                                                                     QUERY PLAN                                                                                                                                                      
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Limit (actual rows=10 loops=1)
   Output: _hyper_3_7_chunk.a, (count(*))
   ->  Finalize GroupAggregate (actual rows=10 loops=1)
         Output: _hyper_3_7_chunk.a, count(*)
         Group Key: _hyper_3_7_chunk.a
         ->  Merge Append (actual rows=11 loops=1)
               Sort Key: _hyper_3_7_chunk.a
               ->  Partial GroupAggregate (actual rows=3 loops=1)
                     Output: _hyper_3_7_chunk.a, PARTIAL count(*)
                     Group Key: _hyper_3_7_chunk.a
                     ->  Custom Scan (DecompressChunk) on _timescaledb_internal._hyper_3_7_chunk (actual rows=3000 loops=1)
                           Output: _hyper_3_7_chunk.a
                           Bulk Decompression: true
                           ->  Index Scan using compress_hyper_4_10_chunk__ts_meta_min_1__ts_meta_max_1__ts_idx on _timescaledb_internal.compress_hyper_4_10_chunk (actual rows=3 loops=1)
                                 Output: compress_hyper_4_10_chunk._ts_meta_count, compress_hyper_4_10_chunk._ts_meta_min_2, compress_hyper_4_10_chunk._ts_meta_max_2, compress_hyper_4_10_chunk.ts, compress_hyper_4_10_chunk._ts_meta_min_1, compress_hyper_4_10_chunk._ts_meta_max_1, compress_hyper_4_10_chunk.a
               ->  Partial GroupAggregate (actual rows=9 loops=1)
                     Output: _hyper_3_9_chunk.a, PARTIAL count(*)
                     Group Key: _hyper_3_9_chunk.a
                     ->  Custom Scan (DecompressChunk) on _timescaledb_internal._hyper_3_9_chunk (actual rows=10 loops=1)
                           Output: _hyper_3_9_chunk.a
                           Bulk Decompression: true
                           ->  Index Scan using compress_hyper_4_11_chunk__ts_meta_min_1__ts_meta_max_1__ts_idx on _timescaledb_internal.compress_hyper_4_11_chunk (actual rows=1 loops=1)
                                 Output: compress_hyper_4_11_chunk._ts_meta_count, compress_hyper_4_11_chunk._ts_meta_min_2, compress_hyper_4_11_chunk._ts_meta_max_2, compress_hyper_4_11_chunk.ts, compress_hyper_4_11_chunk._ts_meta_min_1, compress_hyper_4_11_chunk._ts_meta_max_1, compress_hyper_4_11_chunk.a
(23 rows)

select a, count(*) from text_table group by a order by a limit 10;
       a       | count 
---------------+-------
               |  1000
 default       |  1000
 different1    |     1
 different10   |     1
 different100  |     1
 different1000 |     1
 different101  |     1
 different102  |     1
 different103  |     1
 different104  |     1
(10 rows)

-- The hash grouping policies do not support the GroupAggregate mode in the
-- reverse order.
explain (verbose, costs off, analyze, timing off, summary off)
select a, count(*) from text_table group by a order by a desc limit 10;
                                                                                                                                                     QUERY PLAN                                                                                                                                                      
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Limit (actual rows=10 loops=1)
   Output: _hyper_3_7_chunk.a, (count(*))
   ->  Finalize GroupAggregate (actual rows=10 loops=1)
         Output: _hyper_3_7_chunk.a, count(*)
         Group Key: _hyper_3_7_chunk.a
         ->  Merge Append (actual rows=11 loops=1)
               Sort Key: _hyper_3_7_chunk.a DESC
               ->  Partial GroupAggregate (actual rows=2 loops=1)
                     Output: _hyper_3_7_chunk.a, PARTIAL count(*)
                     Group Key: _hyper_3_7_chunk.a
                     ->  Custom Scan (DecompressChunk) on _timescaledb_internal._hyper_3_7_chunk (actual rows=2001 loops=1)
                           Output: _hyper_3_7_chunk.a
                           Reverse: true
                           Bulk Decompression: true
                           ->  Index Scan Backward using compress_hyper_4_10_chunk__ts_meta_min_1__ts_meta_max_1__ts_idx on _timescaledb_internal.compress_hyper_4_10_chunk (actual rows=3 loops=1)
                                 Output: compress_hyper_4_10_chunk._ts_meta_count, compress_hyper_4_10_chunk._ts_meta_min_2, compress_hyper_4_10_chunk._ts_meta_max_2, compress_hyper_4_10_chunk.ts, compress_hyper_4_10_chunk._ts_meta_min_1, compress_hyper_4_10_chunk._ts_meta_max_1, compress_hyper_4_10_chunk.a
               ->  Partial GroupAggregate (actual rows=10 loops=1)
                     Output: _hyper_3_9_chunk.a, PARTIAL count(*)
                     Group Key: _hyper_3_9_chunk.a
                     ->  Custom Scan (DecompressChunk) on _timescaledb_internal._hyper_3_9_chunk (actual rows=1509 loops=1)
                           Output: _hyper_3_9_chunk.a
                           Reverse: true
                           Bulk Decompression: true
                           ->  Index Scan Backward using compress_hyper_4_11_chunk__ts_meta_min_1__ts_meta_max_1__ts_idx on _timescaledb_internal.compress_hyper_4_11_chunk (actual rows=2 loops=1)
                                 Output: compress_hyper_4_11_chunk._ts_meta_count, compress_hyper_4_11_chunk._ts_meta_min_2, compress_hyper_4_11_chunk._ts_meta_max_2, compress_hyper_4_11_chunk.ts, compress_hyper_4_11_chunk._ts_meta_min_1, compress_hyper_4_11_chunk._ts_meta_max_1, compress_hyper_4_11_chunk.a
(25 rows)

select a, count(*) from text_table group by a order by a desc limit 10;
            a            | count 
-------------------------+-------
 ¤                       |  1000
 same-with-nulls         |   500
 same                    |  1000
 different-with-nulls999 |     1
 different-with-nulls997 |     1
 different-with-nulls995 |     1
 different-with-nulls993 |     1
 different-with-nulls991 |     1
 different-with-nulls99  |     1
 different-with-nulls989 |     1
(10 rows)

-- with NULLS FIRST
select count(decompress_chunk(x)) from show_chunks('text_table') x;
 count 
-------
     2
(1 row)

alter table text_table set (timescaledb.compress,
    timescaledb.compress_segmentby = '', timescaledb.compress_orderby = 'a nulls first');
select count(compress_chunk(x)) from show_chunks('text_table') x;
 count 
-------
     2
(1 row)

select a, count(*) from text_table group by a order by a nulls first limit 10;
       a       | count 
---------------+-------
 ¤             |  1000
               |  1000
 default       |  1000
 different1    |     1
 different10   |     1
 different100  |     1
 different1000 |     1
 different101  |     1
 different102  |     1
 different103  |     1
(10 rows)

-- TODO verify that this works with the serialized hash grouping strategy
select ts, a, count(*) from text_table group by ts, a order by ts, a limit 10;
 ts |       a       | count 
----+---------------+-------
  0 | default       |  1000
  1 |               |  1000
  2 | same          |  1000
  3 | different1    |     1
  3 | different10   |     1
  3 | different100  |     1
  3 | different1000 |     1
  3 | different101  |     1
  3 | different102  |     1
  3 | different103  |     1
(10 rows)

select a, ts, count(*) from text_table group by a, ts order by a desc, ts desc limit 10;
            a            | ts | count 
-------------------------+----+-------
 ¤                       |  5 |   500
 ¤                       |  4 |   500
 same-with-nulls         |  4 |   500
 same                    |  2 |  1000
 different-with-nulls999 |  5 |     1
 different-with-nulls997 |  5 |     1
 different-with-nulls995 |  5 |     1
 different-with-nulls993 |  5 |     1
 different-with-nulls991 |  5 |     1
 different-with-nulls99  |  5 |     1
(10 rows)

reset max_parallel_workers_per_gather;
reset timescaledb.debug_require_vector_agg;
reset enable_hashagg;
