-- This file and its contents are licensed under the Timescale License.
-- Please see the included NOTICE for copyright information and
-- LICENSE-TIMESCALE for a copy of the license.

-- Need to be super user to create extension and add data nodes
\c :TEST_DBNAME :ROLE_CLUSTER_SUPERUSER;

\unset ECHO
\o /dev/null
\ir include/debugsupport.sql
\ir include/filter_exec.sql
\ir include/remote_exec.sql
\o
\set ECHO all

\set DATA_NODE_1 :TEST_DBNAME _1
\set DATA_NODE_2 :TEST_DBNAME _2
\set DATA_NODE_3 :TEST_DBNAME _3
\set DATA_NODE_4 :TEST_DBNAME _4
\set TABLESPACE_1 :TEST_DBNAME _1
\set TABLESPACE_2 :TEST_DBNAME _2
SELECT
    test.make_tablespace_path(:'TEST_TABLESPACE1_PREFIX', :'TEST_DBNAME') AS spc1path,
    test.make_tablespace_path(:'TEST_TABLESPACE2_PREFIX', :'TEST_DBNAME') AS spc2path
\gset

SELECT node_name, database, node_created, database_created, extension_created
FROM (
  SELECT (add_data_node(name, host => 'localhost', DATABASE => name)).*
  FROM (VALUES (:'DATA_NODE_1'), (:'DATA_NODE_2'), (:'DATA_NODE_3')) v(name)
) a;

GRANT USAGE ON FOREIGN SERVER :DATA_NODE_1, :DATA_NODE_2, :DATA_NODE_3 TO PUBLIC;

-- View to see dimension partitions. Note RIGHT JOIN to see that
-- dimension partitions are cleaned up (deleted) properly.
CREATE VIEW hypertable_partitions AS
SELECT table_name, dimension_id, range_start, data_nodes
FROM _timescaledb_catalog.hypertable h
INNER JOIN _timescaledb_catalog.dimension d ON (d.hypertable_id = h.id)
RIGHT JOIN _timescaledb_catalog.dimension_partition dp ON (dp.dimension_id = d.id)
ORDER BY dimension_id, range_start;
GRANT SELECT ON hypertable_partitions TO :ROLE_1;

-- Import testsupport.sql file to data nodes
\unset ECHO
\o /dev/null
\c :DATA_NODE_1
SET client_min_messages TO ERROR;
\ir :TEST_SUPPORT_FILE
\c :DATA_NODE_2
SET client_min_messages TO ERROR;
\ir :TEST_SUPPORT_FILE
\c :DATA_NODE_3
SET client_min_messages TO ERROR;
\ir :TEST_SUPPORT_FILE
\c :TEST_DBNAME :ROLE_CLUSTER_SUPERUSER;
\o
SET client_min_messages TO NOTICE;
\set ECHO all

GRANT CREATE ON SCHEMA public TO :ROLE_1;
SET ROLE :ROLE_1;
--Ensure INSERTs use DataNodeDispatch. DataNodeCopy is tested later
SET timescaledb.enable_distributed_insert_with_copy=false;

-- Verify lack of tables
SELECT node_name FROM timescaledb_information.data_nodes ORDER BY node_name;

\set ON_ERROR_STOP 0
-- Test that one cannot directly create TimescaleDB foreign tables
CREATE FOREIGN TABLE foreign_table (time timestamptz, device int, temp float) SERVER :DATA_NODE_1;
\set ON_ERROR_STOP 1

-- Create distributed hypertables. Add a trigger and primary key
-- constraint to test how those work
CREATE TABLE disttable(time timestamptz, device int CHECK (device > 0), color int, temp float, PRIMARY KEY (time,device));

SELECT * FROM create_distributed_hypertable('disttable', 'time', 'device', 1);

-- Increase the number of partitions. Expect warning since still too
-- low. Dimension partitions should be updated to reflect new
-- partitioning.
SELECT * FROM hypertable_partitions WHERE table_name = 'disttable';
SELECT * FROM set_number_partitions('disttable', 2);
SELECT * FROM hypertable_partitions WHERE table_name = 'disttable';

-- Set number of partitions equal to the number of servers should not
-- raise a warning.
SELECT * FROM hypertable_partitions WHERE table_name = 'disttable';
SELECT * FROM set_number_partitions('disttable', 3, 'device');
SELECT * FROM hypertable_partitions WHERE table_name = 'disttable';

-- Show the number of slices
SELECT h.table_name, d.column_name, d.num_slices
FROM _timescaledb_catalog.hypertable h, _timescaledb_catalog.dimension d
WHERE h.id = d.hypertable_id
AND h.table_name = 'disttable';

-- This table tests both 1-dimensional tables and under-replication
-- (replication_factor > num_data_nodes).
CREATE TABLE underreplicated(time timestamptz, device int, temp float);

\set ON_ERROR_STOP 0
-- can't create an under-replicated hypertable
SELECT * FROM create_hypertable('underreplicated', 'time', replication_factor => 4);
\set ON_ERROR_STOP 1

RESET ROLE;
SELECT node_name, database, node_created, database_created, extension_created
FROM add_data_node(:'DATA_NODE_4', host => 'localhost', database => :'DATA_NODE_4');
GRANT USAGE ON FOREIGN SERVER :DATA_NODE_4 TO PUBLIC;
GRANT CREATE ON SCHEMA public TO :ROLE_1;
SET ROLE :ROLE_1;
SELECT * FROM create_hypertable('underreplicated', 'time', replication_factor => 4);

-- test that attaching a data node to an existing hypertable with
-- repartition=>false does not change the number of partitions when
-- number of partitions is greater than number of data nodes.
SELECT * FROM set_number_partitions('disttable', 8, 'device');
SELECT * FROM hypertable_partitions WHERE table_name = 'disttable';
SELECT attach_data_node(:'DATA_NODE_4', 'disttable', repartition => false);
SELECT * FROM hypertable_partitions WHERE table_name = 'disttable';

--create new session to clear out connections
\c :TEST_DBNAME :ROLE_CLUSTER_SUPERUSER;
SELECT * FROM delete_data_node(:'DATA_NODE_4', force => true, drop_database => true, repartition => false);
SET ROLE :ROLE_1;
-- Deleting a data node should also not change the number of
-- partitions with repartition=>false
SELECT * FROM hypertable_partitions WHERE table_name = 'disttable';

-- reset to 3 partitions
SELECT * FROM set_number_partitions('disttable', 3, 'device');
SELECT * FROM hypertable_partitions WHERE table_name = 'disttable';

CREATE OR REPLACE FUNCTION test_trigger()
    RETURNS TRIGGER LANGUAGE PLPGSQL AS
$BODY$
DECLARE
    cnt INTEGER;
BEGIN
    SELECT count(*) INTO cnt FROM public.disttable;
    RAISE WARNING 'FIRING trigger when: % level: % op: % cnt: % trigger_name %',
        tg_when, tg_level, tg_op, cnt, tg_name;

    IF TG_OP = 'DELETE' THEN
        RETURN OLD;
    END IF;
    RETURN NEW;
END
$BODY$;

-- Create the trigger function on the data nodes:
CALL distributed_exec($$
CREATE OR REPLACE FUNCTION test_trigger()
    RETURNS TRIGGER LANGUAGE PLPGSQL AS
$BODY$
DECLARE
    cnt INTEGER;
BEGIN
    SELECT count(*) INTO cnt FROM public.disttable;
    RAISE WARNING 'FIRING trigger when: % level: % op: % cnt: % trigger_name %',
        tg_when, tg_level, tg_op, cnt, tg_name;

    IF TG_OP = 'DELETE' THEN
        RETURN OLD;
    END IF;
    RETURN NEW;
END
$BODY$;
$$);

CREATE TRIGGER _0_test_trigger_insert
    BEFORE INSERT ON disttable
    FOR EACH ROW EXECUTE FUNCTION test_trigger();

SELECT * FROM _timescaledb_catalog.hypertable_data_node ORDER BY 1,2,3;
SELECT * FROM _timescaledb_catalog.chunk_data_node ORDER BY 1,2,3;

-- The constraints, indexes, and triggers on the hypertable
SELECT * FROM test.show_constraints('disttable');
SELECT * FROM test.show_indexes('disttable');
SELECT * FROM test.show_triggers('disttable');

-- Drop a column. This will make the attribute numbers of the
-- hypertable's root relation differ from newly created chunks. It is
-- a way to test that we properly handle attributed conversion between
-- the root table and chunks
ALTER TABLE disttable DROP COLUMN color;

-- EXPLAIN some inserts to see what plans and explain output for
-- remote inserts look like
EXPLAIN (COSTS FALSE)
INSERT INTO disttable VALUES
       ('2017-01-01 06:01', 1, 1.1);

EXPLAIN (VERBOSE, COSTS FALSE)
INSERT INTO disttable VALUES
       ('2017-01-01 06:01', 1, 1.1);

-- Create some chunks through insertion
INSERT INTO disttable VALUES
       ('2017-01-01 06:01', 1, 1.1),
       ('2017-01-01 09:11', 3, 2.1),
       ('2017-01-01 09:21', 3, 2.2),
       ('2017-01-01 08:11', 3, 2.3),
       ('2017-01-01 08:01', 1, 1.2),
       ('2017-01-02 08:01', 2, 1.3),
       ('2017-01-02 09:01', 2, 1.4),
       ('2017-01-02 08:21', 2, 1.5),
       ('2018-07-02 08:01', 87, 1.6),
       ('2018-07-02 09:01', 87, 1.4),
       ('2018-07-02 09:21', 87, 1.8),
       ('2018-07-01 06:01', 13, 1.4),
       ('2018-07-01 06:21', 13, 1.5),
       ('2018-07-01 07:01', 13, 1.4),
       ('2018-07-01 09:11', 90, 2.7),
       ('2018-07-01 08:01', 29, 1.5),
       ('2018-07-01 09:21', 90, 2.8),
       ('2018-07-01 08:21', 29, 1.2);

-- EXPLAIN some updates/deletes to see what plans and explain output for
-- remote operations look like
EXPLAIN (VERBOSE, COSTS FALSE)
UPDATE disttable SET temp = 3.7 WHERE device = 1;

EXPLAIN (VERBOSE, COSTS FALSE)
DELETE FROM disttable WHERE device = 1;

-- Test distributed ANALYZE.
--

-- First show no statistics
-- reltuples is initially -1 before any VACUUM/ANALYZE has been run on PG14
SELECT relname, relkind, CASE WHEN reltuples > 0 THEN reltuples ELSE 0 END AS reltuples, relpages
FROM pg_class
WHERE oid = 'disttable'::regclass;

SELECT relname, relkind, CASE WHEN reltuples > 0 THEN reltuples ELSE 0 END AS reltuples, relpages
FROM pg_class cl, (SELECT show_chunks AS chunk FROM show_chunks('disttable')) ch
WHERE cl.oid = ch.chunk::regclass;

ANALYZE disttable;

-- Show updated statistics
SELECT relname, relkind, CASE WHEN reltuples > 0 THEN reltuples ELSE 0 END AS reltuples, relpages
FROM pg_class
WHERE oid = 'disttable'::regclass;

SELECT relname, relkind, reltuples, relpages
FROM pg_class cl, (SELECT show_chunks AS chunk FROM show_chunks('disttable')) ch
WHERE cl.oid = ch.chunk::regclass;

-- Test distributed VACUUM.
--
VACUUM (FULL, ANALYZE) disttable;
VACUUM FULL disttable;
VACUUM disttable;
\set ON_ERROR_STOP 0
-- VACUUM VERBOSE is not supported at the moment
VACUUM VERBOSE disttable;
\set ON_ERROR_STOP 1

-- Test prepared statement
PREPARE dist_insert (timestamptz, int, float) AS
INSERT INTO disttable VALUES ($1, $2, $3);

EXECUTE dist_insert ('2017-01-01 06:05', 1, 1.4);

-- Show chunks created
SELECT (_timescaledb_functions.show_chunk(show_chunks)).*
FROM show_chunks('disttable');

-- Show that there are assigned node_chunk_id:s in chunk data node mappings
SELECT * FROM _timescaledb_catalog.chunk_data_node ORDER BY 1,2,3;

SELECT * FROM hypertable_partitions;
-- Show that chunks are created on data nodes and that each data node
-- has their own unique slice in the space (device) dimension.
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
SELECT (_timescaledb_functions.show_chunk(show_chunks)).*
FROM show_chunks('disttable');
$$);
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
SELECT * FROM disttable;
$$);

SELECT node_name FROM timescaledb_information.data_nodes ORDER BY node_name;
SELECT * FROM hypertable_detailed_size('disttable') ORDER BY node_name;

-- Show what some queries would look like on the frontend
EXPLAIN (VERBOSE, COSTS FALSE)
SELECT * FROM disttable;

SELECT * FROM disttable;

EXPLAIN (VERBOSE, COSTS FALSE)
SELECT time_bucket('3 hours', time) AS time, device, avg(temp) AS avg_temp
FROM disttable GROUP BY 1, 2
ORDER BY 1;

-- Execute some queries on the frontend and return the results
SELECT * FROM disttable;

SELECT time_bucket('3 hours', time) AS time, device, avg(temp) AS avg_temp
FROM disttable
GROUP BY 1, 2
ORDER BY 1;

SELECT time_bucket('3 hours', time) AS time, device, avg(temp) AS avg_temp
FROM disttable GROUP BY 1, 2
HAVING avg(temp) > 1.2
ORDER BY 1;

SELECT time_bucket('3 hours', time) AS time, device, avg(temp) AS avg_temp
FROM disttable
WHERE temp > 2
GROUP BY 1, 2
HAVING avg(temp) > 1.2
ORDER BY 1;

-- Test AsyncAppend when using min/max aggregates
EXPLAIN (VERBOSE, COSTS FALSE)
SELECT max(temp)
FROM disttable;

SELECT max(temp)
FROM disttable;

-- Test turning off async append
SET timescaledb.enable_async_append = OFF;

EXPLAIN (VERBOSE, COSTS FALSE)
SELECT max(temp)
FROM disttable;

SET timescaledb.enable_async_append = ON;

EXPLAIN (VERBOSE, COSTS FALSE)
SELECT min(temp), max(temp)
FROM disttable;

SELECT min(temp), max(temp)
FROM disttable;

-- Test AsyncAppend when using window functions
EXPLAIN (VERBOSE, COSTS FALSE)
SELECT device, temp, avg(temp) OVER (PARTITION BY device)
FROM disttable
ORDER BY device, temp;

SELECT device, temp, avg(temp) OVER (PARTITION BY device)
FROM disttable
ORDER BY device, temp;

-- Test remote explain

-- Make sure that chunks_in function only expects one-dimensional integer arrays
\set ON_ERROR_STOP 0
SELECT "time" FROM public.disttable WHERE _timescaledb_functions.chunks_in(public.disttable.*, ARRAY[[2], [1]])
ORDER BY "time" DESC NULLS FIRST LIMIT 1;
\set ON_ERROR_STOP 1

SET timescaledb.enable_remote_explain = ON;

-- Check that datanodes use ChunkAppend plans with chunks_in function in the
-- "Remote SQL" when using max(time).
EXPLAIN (VERBOSE, COSTS FALSE)
SELECT max(time)
FROM disttable;

EXPLAIN (VERBOSE, COSTS FALSE)
SELECT max(temp)
FROM disttable;

-- Don't remote explain if there is no VERBOSE flag
EXPLAIN (COSTS FALSE)
SELECT max(temp)
FROM disttable;

-- Test additional EXPLAIN flags
EXPLAIN (ANALYZE, VERBOSE, COSTS FALSE, BUFFERS OFF, TIMING OFF, SUMMARY OFF)
SELECT max(temp)
FROM disttable;

-- The constraints, indexes, and triggers on foreign chunks. Only
-- check constraints should recurse to foreign chunks (although they
-- aren't enforced on a foreign table)
SELECT st."Child" as chunk_relid, test.show_constraints((st)."Child")
FROM test.show_subtables('disttable') st;
SELECT st."Child" as chunk_relid, test.show_indexes((st)."Child")
FROM test.show_subtables('disttable') st;
SELECT st."Child" as chunk_relid, test.show_triggers((st)."Child")
FROM test.show_subtables('disttable') st;

-- Check that the chunks are assigned data nodes
SELECT * FROM _timescaledb_catalog.chunk_data_node ORDER BY 1,2,3;

-- Adding a new trigger should not recurse to foreign chunks
CREATE TRIGGER _1_test_trigger_insert
    AFTER INSERT ON disttable
    FOR EACH ROW EXECUTE FUNCTION test_trigger();

SELECT st."Child" as chunk_relid, test.show_triggers((st)."Child")
FROM test.show_subtables('disttable') st;

-- Check that we can create indexes on distributed hypertables and
-- that they don't recurse to foreign chunks
CREATE INDEX ON disttable (time, device);

SELECT * FROM test.show_indexes('disttable');
SELECT st."Child" as chunk_relid, test.show_indexes((st)."Child")
FROM test.show_subtables('disttable') st;

-- No index mappings should exist either
SELECT * FROM _timescaledb_catalog.chunk_index;

-- Check that creating columns work
ALTER TABLE disttable ADD COLUMN "Color" int;

SELECT * FROM test.show_columns('disttable');
SELECT st."Child" as chunk_relid, test.show_columns((st)."Child")
FROM test.show_subtables('disttable') st;

-- Adding a new unique constraint should not recurse to foreign
-- chunks, but a check constraint should
ALTER TABLE disttable ADD CONSTRAINT disttable_color_unique UNIQUE (time, device, "Color");
ALTER TABLE disttable ADD CONSTRAINT disttable_temp_non_negative CHECK (temp > 0.0);

SELECT st."Child" as chunk_relid, test.show_constraints((st)."Child")
FROM test.show_subtables('disttable') st;

SELECT cc.*
FROM (SELECT (_timescaledb_functions.show_chunk(show_chunks)).*
      FROM show_chunks('disttable')) c,
      _timescaledb_catalog.chunk_constraint cc
WHERE c.chunk_id = cc.chunk_id;

-- Show contents after re-adding column
SELECT * FROM disttable;

-- Test INSERTS with RETURNING. Since we previously dropped a column
-- on the hypertable, this also tests that we handle conversion of the
-- attribute numbers in the RETURNING clause, since they now differ
-- between the hypertable root relation and the chunk currently
-- RETURNING from.
INSERT INTO disttable (time, device, "Color", temp)
VALUES ('2017-09-02 06:09', 4, 1, 9.8)
RETURNING time, "Color", temp;

INSERT INTO disttable (time, device, "Color", temp)
VALUES ('2017-09-03 06:18', 9, 3, 8.7)
RETURNING 1;

-- On conflict
INSERT INTO disttable (time, device, "Color", temp)
VALUES ('2017-09-02 06:09', 6, 2, 10.5)
ON CONFLICT DO NOTHING;

SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
SELECT (_timescaledb_functions.show_chunk(show_chunks)).*
FROM show_chunks('disttable');
$$);

-- Show new row and that conflicting row is not inserted
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
SELECT * FROM disttable;
$$);


\set ON_ERROR_STOP 0

-- ON CONFLICT DO NOTHING only works when index inference is omitted
\set VERBOSITY default
INSERT INTO disttable
VALUES ('2017-09-02 06:09', 6)
ON CONFLICT(time,device) DO NOTHING;

INSERT INTO disttable
VALUES ('2017-09-02 06:09', 6)
ON CONFLICT(time,device,"Color") DO NOTHING;

INSERT INTO disttable
VALUES ('2017-09-02 06:09', 6)
ON CONFLICT ON CONSTRAINT disttable_color_unique DO NOTHING;
\set VERBOSITY terse

SELECT * FROM disttable ORDER BY disttable;

-- ON CONFLICT only works with DO NOTHING for now
INSERT INTO disttable (time, device, "Color", temp)
VALUES ('2017-09-09 08:13', 7, 3, 27.5)
ON CONFLICT (time) DO UPDATE SET temp = 3.2;

-- Test that an INSERT that would create a chunk does not work on a
-- data node
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1'], $$
    INSERT INTO disttable VALUES ('2019-01-02 12:34', 1, 2, 9.3)
$$);
\set ON_ERROR_STOP 1

-- However, INSERTs on a data node that does not create a chunk works.
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1'], $$
    INSERT INTO disttable VALUES ('2017-09-03 06:09', 1, 2, 9.3)
$$);

-- Test updates
UPDATE disttable SET "Color" = 4 WHERE "Color" = 3;
SELECT * FROM disttable;

WITH devices AS (
     SELECT DISTINCT device FROM disttable ORDER BY device
)
UPDATE disttable SET "Color" = 2 WHERE device = (SELECT device FROM devices LIMIT 1);

\set ON_ERROR_STOP 0
-- Updates referencing non-existing column
UPDATE disttable SET device = 4 WHERE no_such_column = 2;
UPDATE disttable SET no_such_column = 4 WHERE device = 2;
-- Update to system column
UPDATE disttable SET tableoid = 4 WHERE device = 2;
\set ON_ERROR_STOP 1

-- Test deletes (no rows deleted)
DELETE FROM disttable WHERE device = 3
RETURNING *;

DELETE FROM disttable WHERE time IS NULL;

-- Test deletes (rows deleted)
DELETE FROM disttable WHERE device = 4
RETURNING *;

-- Query to show that rows are deleted
SELECT * FROM disttable;

-- Ensure rows are deleted on the data nodes
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
SELECT * FROM disttable;
$$);

-- Test TRUNCATE
TRUNCATE disttable;

-- No data should remain
SELECT * FROM disttable;

-- Metadata and tables cleaned up
SELECT * FROM _timescaledb_catalog.chunk;
SELECT * FROM show_chunks('disttable');

-- Also cleaned up remotely
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
SELECT * FROM _timescaledb_catalog.chunk;
SELECT * FROM show_chunks('disttable');
SELECT * FROM disttable;
$$);

-- The hypertable view also shows no chunks and no data
SELECT * FROM timescaledb_information.hypertables
ORDER BY hypertable_schema, hypertable_name;

-- Test underreplicated chunk warning
INSERT INTO underreplicated VALUES ('2017-01-01 06:01', 1, 1.1),
                                   ('2017-01-02 07:01', 2, 3.5);

SELECT * FROM _timescaledb_catalog.chunk_data_node ORDER BY 1,2,3;
SELECT (_timescaledb_functions.show_chunk(show_chunks)).*
FROM show_chunks('underreplicated');

-- Show chunk data node mappings
SELECT * FROM _timescaledb_catalog.chunk_data_node ORDER BY 1,2,3;

-- Show that chunks are created on remote data nodes and that all
-- data nodes/chunks have the same data due to replication
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
SELECT (_timescaledb_functions.show_chunk(show_chunks)).*
FROM show_chunks('underreplicated');
$$);

SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
SELECT * FROM underreplicated;
$$);

-- Test updates
UPDATE underreplicated SET temp = 2.0 WHERE device = 2
RETURNING time, temp, device;

SELECT * FROM underreplicated;

-- Show that all replica chunks are updated
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
SELECT * FROM underreplicated;
$$);

DELETE FROM underreplicated WHERE device = 2
RETURNING *;

-- Ensure deletes across all data nodes
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
SELECT * FROM underreplicated;
$$);

-- Test hypertable creation fails on distributed error
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_3'], $$
CREATE TABLE remotetable(time timestamptz PRIMARY KEY, id int, cost float);
SELECT * FROM underreplicated;
$$);

\set ON_ERROR_STOP 0
CREATE TABLE remotetable(time timestamptz PRIMARY KEY, device int CHECK (device > 0), color int, temp float);
SELECT * FROM create_hypertable('remotetable', 'time', replication_factor => 1);

-- Test distributed_hypertable creation fails with replication factor 0
CREATE TABLE remotetable2(time timestamptz PRIMARY KEY, device int CHECK (device > 0), color int, temp float);
SELECT * FROM create_distributed_hypertable('remotetable2', 'time', replication_factor => 0);
\set ON_ERROR_STOP 1

SELECT * FROM timescaledb_information.hypertables
ORDER BY hypertable_schema, hypertable_name;

-- Test distributed hypertable creation with many parameters
\c :TEST_DBNAME :ROLE_SUPERUSER

--Ensure INSERTs use DataNodeDispatch.
SET timescaledb.enable_distributed_insert_with_copy=false;
CREATE SCHEMA "T3sTSch";
CREATE SCHEMA "Table\\Schema";
CREATE SCHEMA "single'schema";
GRANT ALL ON SCHEMA "T3sTSch" TO :ROLE_1;
GRANT ALL ON SCHEMA "Table\\Schema" TO :ROLE_1;
GRANT ALL ON SCHEMA "single'schema" TO :ROLE_1;
SET ROLE :ROLE_1;
CREATE TABLE "Table\\Schema"."Param_Table"("time Col %#^#@$#" timestamptz, __region text, reading float);
SELECT * FROM create_distributed_hypertable('"Table\\Schema"."Param_Table"', 'time Col %#^#@$#', partitioning_column => '__region',
associated_schema_name => 'T3sTSch', associated_table_prefix => 'test*pre_', chunk_time_interval => interval '1 week',
create_default_indexes => FALSE, if_not_exists => TRUE, replication_factor => 2,
data_nodes => ARRAY[:'DATA_NODE_2', :'DATA_NODE_3']);

-- Test detach and attach data node
SELECT * FROM detach_data_node(:'DATA_NODE_2', '"Table\\Schema"."Param_Table"', force => true, drop_remote_data => true);

-- Test attach_data_node. First show dimensions and currently attached
-- servers.  The number of slices in the space dimension should equal
-- the number of servers since we didn't explicitly specify
-- number_partitions
SELECT h.table_name, d.column_name, d.num_slices
FROM _timescaledb_catalog.hypertable h, _timescaledb_catalog.dimension d
WHERE h.id = d.hypertable_id
AND h.table_name = 'Param_Table'
ORDER BY 1, 2, 3;

SELECT h.table_name, hdn.node_name
FROM _timescaledb_catalog.hypertable h, _timescaledb_catalog.hypertable_data_node hdn
WHERE h.id = hdn.hypertable_id
AND h.table_name = 'Param_Table'
ORDER BY 1, 2;

SELECT * FROM attach_data_node(:'DATA_NODE_1', '"Table\\Schema"."Param_Table"');

-- Show updated metadata after attach
SELECT h.table_name, d.column_name, d.num_slices
FROM _timescaledb_catalog.hypertable h, _timescaledb_catalog.dimension d
WHERE h.id = d.hypertable_id
AND h.table_name = 'Param_Table'
ORDER BY 1, 2, 3;

SELECT h.table_name, hdn.node_name
FROM _timescaledb_catalog.hypertable h, _timescaledb_catalog.hypertable_data_node hdn
WHERE h.id = hdn.hypertable_id
AND h.table_name = 'Param_Table'
ORDER BY 1, 2;

-- Attach another data node but do not auto-repartition, i.e.,
-- increase the number of slices.
SELECT * FROM attach_data_node(:'DATA_NODE_2', '"Table\\Schema"."Param_Table"', repartition => false);

-- Number of slices should not be increased
SELECT h.table_name, d.column_name, d.num_slices
FROM _timescaledb_catalog.hypertable h, _timescaledb_catalog.dimension d
WHERE h.id = d.hypertable_id
AND h.table_name = 'Param_Table'
ORDER BY 1, 2, 3;

-- Manually increase the number of partitions
SELECT * FROM set_number_partitions('"Table\\Schema"."Param_Table"', 4);

-- Verify hypertables on all data nodes
SELECT * FROM _timescaledb_catalog.hypertable;
SELECT * FROM _timescaledb_catalog.dimension;
SELECT * FROM test.show_triggers('"Table\\Schema"."Param_Table"');

SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
SELECT * FROM _timescaledb_catalog.hypertable;
SELECT * FROM _timescaledb_catalog.dimension;
SELECT t.tgname, t.tgtype, t.tgfoid::regproc
FROM pg_trigger t, pg_class c WHERE c.relname = 'Param_Table' AND t.tgrelid = c.oid;
$$);

-- Verify that repartitioning works as expected on detach_data_node
SELECT * FROM detach_data_node(:'DATA_NODE_1', '"Table\\Schema"."Param_Table"', repartition => true);
SELECT h.table_name, d.column_name, d.num_slices
FROM _timescaledb_catalog.hypertable h, _timescaledb_catalog.dimension d
WHERE h.id = d.hypertable_id
AND h.table_name = 'Param_Table';
SELECT * FROM detach_data_node(:'DATA_NODE_2', '"Table\\Schema"."Param_Table"', force => true, repartition => false);
SELECT h.table_name, d.column_name, d.num_slices
FROM _timescaledb_catalog.hypertable h, _timescaledb_catalog.dimension d
WHERE h.id = d.hypertable_id
AND h.table_name = 'Param_Table';

-- Test multi-dimensional hypertable. The add_dimension() command
-- should be propagated to backends.
CREATE TABLE dimented_table (time timestamptz, column1 int, column2 timestamptz, column3 int);
SELECT * FROM create_distributed_hypertable('dimented_table', 'time', partitioning_column => 'column1', number_partitions  => 4, replication_factor => 1, data_nodes => ARRAY[:'DATA_NODE_1']);
-- Create one chunk to block add_dimension
INSERT INTO dimented_table VALUES('2017-01-01 06:01', 1, '2017-01-01 08:01', 1);

CREATE VIEW dimented_table_slices AS
SELECT c.id AS chunk_id, c.hypertable_id, ds.dimension_id, cc.dimension_slice_id, c.schema_name AS
       chunk_schema, c.table_name AS chunk_table, ds.range_start, ds.range_end
FROM _timescaledb_catalog.chunk c
INNER JOIN _timescaledb_catalog.hypertable h ON (c.hypertable_id = h.id)
INNER JOIN _timescaledb_catalog.dimension td ON (h.id = td.hypertable_id)
INNER JOIN _timescaledb_catalog.dimension_slice ds ON (ds.dimension_id = td.id)
INNER JOIN _timescaledb_catalog.chunk_constraint cc ON (cc.dimension_slice_id = ds.id AND cc.chunk_id = c.id)
WHERE h.table_name = 'dimented_table'
ORDER BY c.id, ds.dimension_id;

SELECT * FROM dimented_table_slices;

-- add_dimension() with existing data
SELECT * FROM add_dimension('dimented_table', 'column2', chunk_time_interval => interval '1 week');
SELECT * FROM dimented_table_slices;

SELECT * FROM add_dimension('dimented_table', 'column3', 4, partitioning_func => '_timescaledb_internal.get_partition_for_key');
SELECT * FROM dimented_table_slices;
SELECT * FROM dimented_table ORDER BY time;

SELECT * FROM _timescaledb_catalog.dimension;
SELECT * FROM attach_data_node(:'DATA_NODE_2', 'dimented_table');

SELECT * FROM _timescaledb_catalog.dimension;

-- ensure data node has new dimensions
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
SELECT * FROM _timescaledb_catalog.dimension;
$$);

--test per-data node queries
-- Create some chunks through insertion
CREATE TABLE disttable_replicated(time timestamptz PRIMARY KEY, device int CHECK (device > 0), temp float, "Color" int);
SELECT * FROM create_hypertable('disttable_replicated', 'time', replication_factor => 2);
INSERT INTO disttable_replicated VALUES
       ('2017-01-01 06:01', 1, 1.1, 1),
       ('2017-01-01 08:01', 1, 1.2, 2),
       ('2018-01-02 08:01', 2, 1.3, 3),
       ('2019-01-01 09:11', 3, 2.1, 4),
       ('2020-01-01 06:01', 5, 1.1, 10),
       ('2020-01-01 08:01', 6, 1.2, 11),
       ('2021-01-02 08:01', 7, 1.3, 12),
       ('2022-01-01 09:11', 8, 2.1, 13);

SELECT * FROM disttable_replicated;

EXPLAIN (VERBOSE, ANALYZE, COSTS FALSE, TIMING FALSE, SUMMARY FALSE)
SELECT * FROM disttable_replicated;

--guc disables the optimization
SET timescaledb.enable_per_data_node_queries = FALSE;
EXPLAIN (VERBOSE, ANALYZE, COSTS FALSE, TIMING FALSE, SUMMARY FALSE)
SELECT * FROM disttable_replicated;
SET timescaledb.enable_per_data_node_queries = TRUE;

--test WHERE clause
EXPLAIN (VERBOSE, ANALYZE, COSTS FALSE, TIMING FALSE, SUMMARY FALSE)
SELECT * FROM disttable_replicated WHERE temp > 2.0;

--test OR
EXPLAIN (VERBOSE, ANALYZE, COSTS FALSE, TIMING FALSE, SUMMARY FALSE)
SELECT * FROM disttable_replicated WHERE temp > 2.0 or "Color" = 11;

--test some chunks excluded
EXPLAIN (VERBOSE, ANALYZE, COSTS FALSE,  TIMING FALSE, SUMMARY FALSE)
SELECT * FROM disttable_replicated WHERE time < '2018-01-01 09:11';

--test all chunks excluded
EXPLAIN (VERBOSE, ANALYZE, COSTS FALSE,  TIMING FALSE, SUMMARY FALSE)
SELECT * FROM disttable_replicated WHERE time < '2002-01-01 09:11';

--test cte
EXPLAIN (VERBOSE, ANALYZE, COSTS FALSE,  TIMING FALSE, SUMMARY FALSE)
WITH cte AS (
    SELECT * FROM disttable_replicated
)
SELECT * FROM cte;

--queries that involve updates/inserts are not optimized
EXPLAIN (VERBOSE, ANALYZE, COSTS FALSE,  TIMING FALSE, SUMMARY FALSE)
WITH devices AS (
     SELECT DISTINCT device FROM disttable_replicated ORDER BY device
)
UPDATE disttable_replicated SET device = 2 WHERE device = (SELECT device FROM devices LIMIT 1);


-- Test inserts with smaller batch size and more tuples to reach full
-- batch
SET timescaledb.max_insert_batch_size=4;

CREATE TABLE twodim (time timestamptz DEFAULT '2019-02-10 10:11', "Color" int DEFAULT 11 CHECK ("Color" > 0), temp float DEFAULT 22.1);
-- Create a replicated table to ensure we handle that case correctly
-- with batching
SELECT * FROM create_hypertable('twodim', 'time', 'Color', 3, replication_factor => 2, data_nodes => ARRAY[:'DATA_NODE_1',:'DATA_NODE_2',:'DATA_NODE_3']);

SELECT * FROM twodim
ORDER BY time;

-- INSERT enough data to stretch across multiple batches per
-- data node. Also return a system column. Although we write tuples to
-- multiple data nodes, the returned tuple should only be the ones in the
-- original insert statement (without the replica tuples).
WITH result AS (
     INSERT INTO twodim VALUES
       ('2017-02-01 06:01', 1, 1.1),
       ('2017-02-01 08:01', 1, 1.2),
       ('2018-02-02 08:01', 2, 1.3),
       ('2019-02-01 09:11', 3, 2.1),
       ('2019-02-02 09:11', 3, 2.1),
       ('2019-02-02 10:01', 5, 1.2),
       ('2019-02-03 11:11', 6, 3.5),
       ('2019-02-04 08:21', 4, 6.6),
       ('2019-02-04 10:11', 7, 7.4),
       ('2019-02-04 12:11', 8, 2.1),
       ('2019-02-05 13:31', 8, 6.3),
       ('2019-02-06 02:11', 5, 1.8),
       ('2019-02-06 01:13', 7, 7.9),
       ('2019-02-06 19:24', 9, 5.9),
       ('2019-02-07 18:44', 5, 9.7),
       ('2019-02-07 20:24', 6, NULL),
       ('2019-02-07 09:33', 7, 9.5),
       ('2019-02-08 08:54', 1, 7.3),
       ('2019-02-08 18:14', 4, 8.2),
       ('2019-02-09 19:23', 8, 9.1)
     RETURNING tableoid = 'twodim'::regclass AS is_tableoid, time, temp, "Color"
) SELECT * FROM result ORDER BY time;

-- Test insert with default values and a batch size of 1.
SET timescaledb.max_insert_batch_size=1;
EXPLAIN (VERBOSE, COSTS OFF, TIMING OFF, SUMMARY OFF)
INSERT INTO twodim DEFAULT VALUES;
INSERT INTO twodim DEFAULT VALUES;

-- Reset the batch size
SET timescaledb.max_insert_batch_size=4;

-- Constraint violation error check
--
-- Execute and filter mentioned data node name in the error message.
\set ON_ERROR_STOP 0
SELECT test.execute_sql_and_filter_data_node_name_on_error($$ INSERT INTO twodim VALUES ('2019-02-10 17:54', 0, 10.2) $$, :'TEST_DBNAME');
\set ON_ERROR_STOP 1

-- Disable batching, reverting to FDW tuple-by-tuple inserts.
-- First EXPLAIN with batching turned on.
EXPLAIN (VERBOSE, COSTS OFF, TIMING OFF, SUMMARY OFF)
INSERT INTO twodim VALUES
       ('2019-02-10 16:23', 5, 7.1),
       ('2019-02-10 17:11', 7, 3.2);

SET timescaledb.max_insert_batch_size=0;

-- Compare without batching
EXPLAIN (VERBOSE, COSTS OFF, TIMING OFF, SUMMARY OFF)
INSERT INTO twodim VALUES
       ('2019-02-10 16:23', 5, 7.1),
       ('2019-02-10 17:11', 7, 3.2);

-- Insert without batching
INSERT INTO twodim VALUES
       ('2019-02-10 16:23', 5, 7.1),
       ('2019-02-10 17:11', 7, 3.2);

-- Check that datanodes use ChunkAppend plans with chunks_in function in the
-- "Remote SQL" when multiple dimensions are involved.
SET timescaledb.enable_remote_explain = ON;
EXPLAIN (VERBOSE, COSTS OFF, TIMING OFF, SUMMARY OFF)
SELECT * FROM twodim
ORDER BY time;
SET timescaledb.enable_remote_explain = OFF;

-- Check results
SELECT * FROM twodim
ORDER BY time;

SELECT count(*) FROM twodim;

-- Show distribution across data nodes
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
SELECT * FROM twodim
ORDER BY time;
SELECT count(*) FROM twodim;
$$);

-- Distributed table with custom type that has no binary output
CREATE TABLE disttable_with_ct(time timestamptz, txn_id rxid, val float, info text);
SELECT * FROM create_hypertable('disttable_with_ct', 'time', replication_factor => 2);

-- Insert data with custom type
INSERT INTO disttable_with_ct VALUES
    ('2019-01-01 01:01', 'ts-1-10-20-30', 1.1, 'a'),
    ('2019-01-01 01:02', 'ts-1-11-20-30', 2.0, repeat('abc', 1000000)); -- TOAST

-- Test queries on distributed table with custom type
SELECT time, txn_id, val, substring(info for 20) FROM disttable_with_ct;

SET timescaledb.enable_connection_binary_data=false;

SELECT time, txn_id, val, substring(info for 20) FROM disttable_with_ct;

-- Test DELETE with replication
DELETE FROM disttable_with_ct WHERE info = 'a';
-- Check if row is gone
SELECT time, txn_id, val, substring(info for 20) FROM disttable_with_ct;
-- Connect to data nodes to see if data is gone

SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
SELECT time, txn_id, val, substring(info for 20) FROM disttable_with_ct;
$$);

-- Test single quote in names
SET SCHEMA 'single''schema';
CREATE TABLE "disttable'quote"(time timestamptz, "device'quote" int, val float, info text);
SELECT public.create_distributed_hypertable(
    'disttable''quote', 'time', 'device''quote', data_nodes => ARRAY[:'DATA_NODE_1']
);

SET SCHEMA 'public';
CREATE TABLE disttable_drop_chunks(time timestamptz, device int CHECK (device > 0), color int, PRIMARY KEY (time,device));
SELECT * FROM create_distributed_hypertable('disttable_drop_chunks', 'time', 'device', number_partitions => 3, replication_factor => 2);

INSERT INTO disttable_drop_chunks VALUES
       ('2017-01-01 06:01', 1, 1.1),
       ('2017-01-01 09:11', 3, 2.1),
       ('2017-01-01 08:01', 1, 1.2),
       ('2017-01-02 08:01', 2, 1.3),
       ('2018-07-02 08:01', 87, 1.6),
       ('2018-07-01 06:01', 13, 1.4),
       ('2018-07-01 09:11', 90, 2.7),
       ('2018-07-01 08:01', 29, 1.5);

-- Show chunks on access node
SELECT (_timescaledb_functions.show_chunk(show_chunks)).*
FROM show_chunks('disttable_drop_chunks');

-- Show chunks on data nodes
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
SELECT (_timescaledb_functions.show_chunk(show_chunks)).*
FROM show_chunks('disttable_drop_chunks');
$$);

SELECT * FROM drop_chunks('disttable_drop_chunks', older_than => '2018-01-01'::timestamptz);

SELECT * FROM disttable_drop_chunks;

SELECT (_timescaledb_functions.show_chunk(show_chunks)).*
FROM show_chunks('disttable_drop_chunks');

SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
SELECT (_timescaledb_functions.show_chunk(show_chunks)).*
FROM show_chunks('disttable_drop_chunks');
$$);

-- test passing newer_than as interval
SELECT * FROM drop_chunks('disttable_drop_chunks', newer_than => interval '10 years');
SELECT * FROM disttable_drop_chunks;

CREATE TABLE "weird nAme\\#^."(time bigint, device int CHECK (device > 0), color int, PRIMARY KEY (time,device));
SELECT * FROM create_distributed_hypertable('"weird nAme\\#^."', 'time', 'device', 3, chunk_time_interval => 100, replication_factor => 2);

INSERT INTO "weird nAme\\#^." VALUES
       (300, 1, 1.1),
       (400, 3, 2.1),
       (350, 1, 1.2);

SELECT * FROM "weird nAme\\#^.";
-- drop chunks using bigint as time
SELECT * FROM drop_chunks('"weird nAme\\#^."', older_than => 1000);
SELECT * FROM "weird nAme\\#^.";

-----------------------------------------------------------------------------------------
-- Test that settings on hypertables are distributed to data nodes
-----------------------------------------------------------------------------------------

DROP TABLE disttable CASCADE;
CREATE TABLE disttable (time bigint, device int, temp float);
SELECT create_distributed_hypertable('disttable', 'time', chunk_time_interval => 1000000::bigint);

-- Show the dimension configuration on data nodes
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
SELECT d.* FROM _timescaledb_catalog.hypertable h, _timescaledb_catalog.dimension d
WHERE h.id = d.hypertable_id AND h.table_name = 'disttable';
$$);
-- Test adding a space dimension. Should apply to data nodes as
-- well. We're setting num_partitions lower than the number of servers
-- and expect a warning.
SELECT * FROM add_dimension('disttable', 'device', 1, partitioning_func => '_timescaledb_internal.get_partition_hash');
CREATE INDEX disttable_device_time_idx ON disttable (device, time);

SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
SELECT d.* FROM _timescaledb_catalog.hypertable h, _timescaledb_catalog.dimension d
WHERE h.id = d.hypertable_id AND h.table_name = 'disttable';
$$);

-- Show that changing dimension settings apply to data nodes
SELECT * FROM set_chunk_time_interval('disttable', 2000000000::bigint);
SELECT * FROM set_number_partitions('disttable', 3);

CREATE OR REPLACE FUNCTION dummy_now() RETURNS BIGINT LANGUAGE SQL IMMUTABLE as  'SELECT 2::BIGINT';
CALL distributed_exec($$
CREATE OR REPLACE FUNCTION dummy_now() RETURNS BIGINT LANGUAGE SQL IMMUTABLE as  'SELECT 2::BIGINT'
$$);

SELECT * FROM set_integer_now_func('disttable', 'dummy_now');

-- Show changes to dimensions
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
SELECT d.* FROM _timescaledb_catalog.hypertable h, _timescaledb_catalog.dimension d
WHERE h.id = d.hypertable_id AND h.table_name = 'disttable';
$$);

-- Tests for using tablespaces with distributed hypertables
\c :TEST_DBNAME :ROLE_CLUSTER_SUPERUSER;
--Ensure INSERTs use DataNodeDispatch.
SET timescaledb.enable_distributed_insert_with_copy=false;

CREATE TABLESPACE :TABLESPACE_1 OWNER :ROLE_1 LOCATION :'spc1path';
CREATE TABLESPACE :TABLESPACE_2 OWNER :ROLE_1 LOCATION :'spc2path';
\set ON_ERROR_STOP 0
SELECT attach_tablespace(:'TABLESPACE_1', 'disttable');
SELECT detach_tablespace(:'TABLESPACE_1', 'disttable');
\set ON_ERROR_STOP 1
SELECT detach_tablespaces('disttable');

-- Continue to use previously attached tablespace, but block attach/detach
CREATE TABLE disttable2(time timestamptz, device int, temp float) TABLESPACE :TABLESPACE_1;
SELECT create_distributed_hypertable('disttable2', 'time', chunk_time_interval => 1000000::bigint);

-- Ensure that table is created on the data nodes without a tablespace
CALL distributed_exec($$
SELECT * FROM show_tablespaces('disttable2');
$$);

INSERT INTO disttable2 VALUES ('2017-01-01 06:01', 1, 1.1);
SELECT * FROM show_chunks('disttable2');

-- Ensure tablespace oid is set to 0 for a foreign table
SELECT reltablespace
FROM pg_class cl, (SELECT show_chunks AS chunk FROM show_chunks('disttable2')) ch
WHERE cl.oid = ch.chunk::regclass;

\set ON_ERROR_STOP 0
SELECT attach_tablespace(:'TABLESPACE_2', 'disttable2');
SELECT detach_tablespace(:'TABLESPACE_2', 'disttable2');
\set ON_ERROR_STOP 1
SELECT detach_tablespaces('disttable2');

SELECT * FROM show_tablespaces('disttable2');

-- Ensure tablespace API works for data nodes
CALL distributed_exec(format($$
SELECT attach_tablespace(%L, 'disttable2');
$$, :'TABLESPACE_2'));
CALL distributed_exec(format($$
SELECT detach_tablespace(%L, 'disttable2');
$$, :'TABLESPACE_2'));
CALL distributed_exec(format($$
SELECT attach_tablespace(%L, 'disttable2');
$$, :'TABLESPACE_2'));
CALL distributed_exec($$
SELECT detach_tablespaces('disttable2');
$$);
DROP TABLE disttable2;

CREATE TABLE disttable2(time timestamptz, device int, temp float) TABLESPACE :TABLESPACE_1;
SELECT create_hypertable('disttable2', 'time', chunk_time_interval => 1000000::bigint, replication_factor => 1);

-- Ensure that table is created on the data nodes without a tablespace
CALL distributed_exec($$
SELECT * FROM show_tablespaces('disttable2');
$$);

INSERT INTO disttable2 VALUES ('2017-01-01 06:01', 1, 1.1);
SELECT * FROM show_chunks('disttable2');

-- Ensure tablespace oid is set to 0 for a foreign table
SELECT reltablespace
FROM pg_class cl, (SELECT show_chunks AS chunk FROM show_chunks('disttable2')) ch
WHERE cl.oid = ch.chunk::regclass;

\set ON_ERROR_STOP 0
SELECT attach_tablespace(:'TABLESPACE_2', 'disttable2');
SELECT detach_tablespace(:'TABLESPACE_2', 'disttable2');
\set ON_ERROR_STOP 1

SELECT * FROM show_tablespaces('disttable2');
DROP TABLE disttable2;

DROP TABLESPACE :TABLESPACE_1;
DROP TABLESPACE :TABLESPACE_2;

-- Make sure table qualified name is used in chunks_in function. Otherwise having a table name same as a column name might yield an error
CREATE TABLE dist_device(time timestamptz, dist_device int, temp float);
SELECT * FROM create_distributed_hypertable('dist_device', 'time');

INSERT INTO dist_device VALUES
       ('2017-01-01 06:01', 1, 1.1),
       ('2017-01-01 09:11', 3, 2.1),
       ('2017-01-01 08:01', 1, 1.2);

EXPLAIN (VERBOSE, COSTS OFF)
SELECT * FROM dist_device;

-- Check that datanodes use ChunkAppend plans with chunks_in function in the
-- "Remote SQL" when only time partitioning is being used.
SET timescaledb.enable_remote_explain = ON;
EXPLAIN (VERBOSE, COSTS OFF, TIMING OFF, SUMMARY OFF)
SELECT "time", dist_device, temp FROM public.dist_device ORDER BY "time" ASC NULLS LAST;

SELECT * FROM dist_device;

-- Test estimating relation size without stats
CREATE TABLE hyper_estimate(time timestamptz, device int, temp float);
SELECT * FROM create_distributed_hypertable('hyper_estimate', 'time', 'device', number_partitions => 3, replication_factor => 1, chunk_time_interval => INTERVAL '7 days');

-- This will enable us to more easily see estimates per chunk
SET timescaledb.enable_per_data_node_queries = false;

-- Estimating chunk progress uses current timestamp so we override it for test purposes
SELECT test.tsl_override_current_timestamptz('2017-11-11 00:00'::timestamptz);

-- Test estimates when backfilling. 3 chunks should be historical and 3 should be considered current when estimating.
-- Note that estimate numbers are way off since we are using shared buffer size as starting point. This will not be
-- an issue in 'production' like env since chunk size should be similar to shared buffer size.
INSERT INTO hyper_estimate VALUES
       ('2017-01-01 06:01', 1, 1.1),
       ('2017-01-01 09:11', 1, 2.1),
       ('2017-01-01 08:01', 1, 1.2),
       ('2017-01-02 08:01', 1, 1.3),
       ('2017-01-02 08:01', 2, 1.6),
       ('2017-01-02 06:01', 2, 1.4),
       ('2017-01-03 01:01', 3, 2),
       ('2017-01-03 01:16', 3, 3),
       ('2017-01-03 01:17', 3, 4),
       ('2018-01-13 01:01', 1, 2),
       ('2018-01-13 01:10', 1, 0.4),
       ('2018-01-13 02:10', 2, 1.4),
       ('2018-01-13 05:01', 2, 2),
       ('2018-01-13 05:50', 2, 4),
       ('2018-01-13 16:01', 3, 2);

-- This will calculate the stats
ANALYZE hyper_estimate;

EXPLAIN (COSTS ON)
SELECT *
FROM hyper_estimate;

-- Let's insert data into a new chunk. This will result in chunk creation.
INSERT INTO hyper_estimate VALUES ('2019-11-11 06:01', 1, 1.1);

-- We have stats for previous chunks so we can interpolate number of rows for the new chunk
EXPLAIN (COSTS ON)
SELECT *
FROM hyper_estimate;

CREATE TABLE devices (
       device_id INTEGER PRIMARY KEY,
       device_name VARCHAR(10)
);

CALL distributed_exec($$
  CREATE TABLE devices(device_id INTEGER PRIMARY KEY, device_name VARCHAR(10))
$$);

INSERT INTO devices VALUES
  (1, 'A001'), (2, 'B015'), (3, 'D821'), (4, 'C561'), (5, 'D765');

CALL distributed_exec($$
  INSERT INTO devices VALUES
    (1, 'A001'), (2, 'B015'), (3, 'D821'), (4, 'C561'), (5, 'D765')
$$);

CREATE TABLE hyper (
  time TIMESTAMPTZ NOT NULL,
  device INTEGER REFERENCES devices(device_id),
  temp FLOAT
);

SELECT * FROM create_distributed_hypertable('hyper', 'time', 'device', 3,
       chunk_time_interval => interval '18 hours'
);

-- Inserting some values should succeed.
INSERT INTO hyper VALUES
       ('2017-01-01 06:01', 1, 1.1),
       ('2017-01-01 09:11', 1, 2.1),
       ('2017-01-01 08:01', 1, 1.2),
       ('2017-01-02 08:01', 1, 1.3),
       ('2017-01-02 08:01', 2, 1.6),
       ('2017-01-02 06:01', 2, 1.4),
       ('2017-01-03 01:01', 3, 2),
       ('2017-01-03 01:16', 3, 3),
       ('2017-01-03 01:17', 3, 4),
       ('2018-01-13 01:01', 1, 2),
       ('2018-01-13 01:10', 1, 0.4),
       ('2018-01-13 02:10', 2, 1.4),
       ('2018-01-13 05:01', 2, 2),
       ('2018-01-13 05:50', 2, 4),
       ('2018-01-13 16:01', 3, 2);

SELECT time_bucket('3 hours', time) AS time, device, avg(temp) AS avg_temp
FROM hyper
GROUP BY 1, 2
HAVING avg(temp) > 1.2
ORDER BY 1;

-- Add some devices on the access node only. Inserts should then fail.
INSERT INTO devices VALUES (6, 'E999');

\set ON_ERROR_STOP 0
INSERT INTO hyper VALUES ('2017-01-01 06:01', 6, 1.1);
\set ON_ERROR_STOP 1

-- Test alter replication factor with data

SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
    SELECT (_timescaledb_functions.show_chunk(show_chunks)).*
    FROM show_chunks('hyper');
$$);

-- Dimension partitions should be updated to account for replication
-- to additional data nodes
SELECT * FROM hypertable_partitions WHERE table_name = 'hyper';
SELECT * FROM set_replication_factor('hyper', 3);
SELECT * FROM hypertable_partitions WHERE table_name = 'hyper';

INSERT INTO hyper VALUES ('2017-01-02 07:11', 1, 1.7);

SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
    SELECT (_timescaledb_functions.show_chunk(show_chunks)).*
    FROM show_chunks('hyper');
$$);

INSERT INTO hyper VALUES ('2017-02-01 06:01', 1, 5.1);

SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
    SELECT (_timescaledb_functions.show_chunk(show_chunks)).*
    FROM show_chunks('hyper');
$$);

SELECT * FROM set_replication_factor('hyper', 2);
SELECT * FROM hypertable_partitions WHERE table_name = 'hyper';

INSERT INTO hyper VALUES ('2017-03-01 06:01', 1, 15.1);

SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
    SELECT (_timescaledb_functions.show_chunk(show_chunks)).*
    FROM show_chunks('hyper');
$$);

SELECT * FROM set_replication_factor('hyper', replication_factor => 2);

INSERT INTO hyper VALUES ('2017-04-01 06:01', 2, 45.1);

SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
    SELECT (_timescaledb_functions.show_chunk(show_chunks)).*
    FROM show_chunks('hyper');
$$);

\set ON_ERROR_STOP 0
SELECT * FROM set_replication_factor('hyper', replication_factor => 4);
\set ON_ERROR_STOP 1

DROP TABLE hyper;
CALL distributed_exec($$
    DROP TABLE devices;
$$);
DROP TABLE devices;

-- Test storage options are distributed to data nodes
--
-- Make sure that options used during CREATE TABLE WITH and CREATE INDEX WITH
-- are properly distributed.
--
CREATE TABLE disttable_with_relopts_1(time timestamptz NOT NULL, device int) WITH (fillfactor=10);
CREATE TABLE disttable_with_relopts_2(time timestamptz NOT NULL, device int) WITH (fillfactor=10, parallel_workers=1);
CREATE TABLE disttable_with_relopts_3(time timestamptz NOT NULL, device int);
CREATE INDEX disttable_with_relopts_3_idx ON disttable_with_relopts_3(device) WITH (fillfactor=20);

SELECT * FROM create_distributed_hypertable('disttable_with_relopts_1', 'time');
SELECT * FROM create_distributed_hypertable('disttable_with_relopts_2', 'time');
SELECT * FROM create_distributed_hypertable('disttable_with_relopts_3', 'time');

INSERT INTO disttable_with_relopts_1 VALUES
       ('2017-01-01 06:01', 1),
       ('2017-01-01 09:11', 3),
       ('2017-01-01 08:01', 1),
       ('2017-01-02 08:01', 2),
       ('2018-07-02 08:01', 87);

INSERT INTO disttable_with_relopts_2 VALUES
       ('2017-01-01 06:01', 1),
       ('2017-01-01 09:11', 3),
       ('2017-01-01 08:01', 1),
       ('2017-01-02 08:01', 2),
       ('2018-07-02 08:01', 87);

SELECT relname, reloptions FROM pg_class WHERE relname = 'disttable_with_relopts_1' ORDER BY relname;
SELECT relname, reloptions FROM pg_class WHERE relname = 'disttable_with_relopts_2' ORDER BY relname;
SELECT relname, reloptions FROM pg_class WHERE relname = 'disttable_with_relopts_3' ORDER BY relname;
SELECT relname, reloptions FROM pg_class WHERE relname = 'disttable_with_relopts_3_idx' ORDER BY relname;

-- Ensure reloptions are not set for distributed hypertable chunks on the AN
SELECT relname, reloptions FROM pg_class WHERE relname IN
(SELECT (_timescaledb_functions.show_chunk(show_chunks)).table_name FROM show_chunks('disttable_with_relopts_1'))
ORDER BY relname;

SELECT relname, reloptions FROM pg_class WHERE relname IN
(SELECT (_timescaledb_functions.show_chunk(show_chunks)).table_name FROM show_chunks('disttable_with_relopts_2'))
ORDER BY relname;

-- Ensure parent tables has proper storage options
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
    SELECT relname, reloptions FROM pg_class WHERE relname = 'disttable_with_relopts_1' ORDER BY relname;
$$);

SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
    SELECT relname, reloptions FROM pg_class WHERE relname = 'disttable_with_relopts_2' ORDER BY relname;
$$);

-- Ensure index has proper storage options set
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
    SELECT relname, reloptions FROM pg_class WHERE relname = 'disttable_with_relopts_3_idx' ORDER BY relname;
$$);

-- Make sure chunks derive parent reloptions
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
    SELECT relname, reloptions FROM pg_class WHERE relname IN
    (SELECT (_timescaledb_functions.show_chunk(show_chunks)).table_name FROM show_chunks('disttable_with_relopts_1'))
    ORDER BY relname;
$$);

SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
    SELECT relname, reloptions FROM pg_class WHERE relname IN
    (SELECT (_timescaledb_functions.show_chunk(show_chunks)).table_name FROM show_chunks('disttable_with_relopts_2'))
    ORDER BY relname;
$$);

-- ALTER TABLE SET/RESET support for distributed hypertable
--
-- SET
ALTER TABLE disttable_with_relopts_1 SET (fillfactor=40);

SELECT relname, reloptions FROM pg_class WHERE relname = 'disttable_with_relopts_1' ORDER BY relname;

-- Ensure chunks are not affected on the AN
SELECT relname, reloptions FROM pg_class WHERE relname IN
(SELECT (_timescaledb_functions.show_chunk(show_chunks)).table_name FROM show_chunks('disttable_with_relopts_1'))
ORDER BY relname;

-- Ensure data node chunks has proper options set
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
    SELECT relname, reloptions FROM pg_class WHERE relname IN
    (SELECT (_timescaledb_functions.show_chunk(show_chunks)).table_name FROM show_chunks('disttable_with_relopts_1'))
    ORDER BY relname;
$$);

-- RESET
ALTER TABLE disttable_with_relopts_1 RESET (fillfactor);

SELECT relname, reloptions FROM pg_class WHERE relname = 'disttable_with_relopts_1' ORDER BY relname;

-- Ensure chunks are not affected on the AN
SELECT relname, reloptions FROM pg_class WHERE relname IN
(SELECT (_timescaledb_functions.show_chunk(show_chunks)).table_name FROM show_chunks('disttable_with_relopts_1'))
ORDER BY relname;

-- Ensure data node chunks has proper options set
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
    SELECT relname, reloptions FROM pg_class WHERE relname IN
    (SELECT (_timescaledb_functions.show_chunk(show_chunks)).table_name FROM show_chunks('disttable_with_relopts_1'))
    ORDER BY relname;
$$);

DROP TABLE disttable_with_relopts_1;
DROP TABLE disttable_with_relopts_2;
DROP TABLE disttable_with_relopts_3;

-- Test SERIAL type column support for distributed hypertables
--
CREATE TABLE disttable_serial(time timestamptz NOT NULL, device int, id1 SERIAL, id2 SMALLSERIAL, id3 BIGSERIAL);
SELECT create_distributed_hypertable('disttable_serial', 'time', 'device');

-- Show created columns (AN and DN tables must be exact)
SELECT * FROM test.show_columns('disttable_serial');
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
	SELECT * FROM test.show_columns('disttable_serial');
$$);

-- Ensure DEFAULT expression is applied on the AN only
SELECT column_name, column_default
FROM information_schema.columns
WHERE table_name  = 'disttable_serial';

SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
	SELECT column_name, column_default
	FROM information_schema.columns
	WHERE table_name  = 'disttable_serial';
$$);

-- Ensure sequences were created on the AN only
INSERT INTO disttable_serial VALUES
       ('2017-01-01 06:01', 1),
       ('2017-01-01 09:11', 3),
       ('2017-01-01 08:01', 1),
       ('2017-01-02 08:01', 2),
       ('2018-07-02 08:01', 87);
SELECT currval('disttable_serial_id1_seq'::regclass),
       currval('disttable_serial_id2_seq'::regclass),
       currval('disttable_serial_id3_seq'::regclass);
\set ON_ERROR_STOP 0
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1'], $$
	SELECT currval('disttable_serial_id1_seq'::regclass);
$$);
\set ON_ERROR_STOP 1

-- Verify that the data is getting spread over multiple data nodes with the
-- serial values being set correctly
SELECT * from disttable_serial ORDER BY id1;

SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
	SELECT * from disttable_serial ORDER BY id1;
$$);

DROP TABLE disttable_serial;

-- Test insert batching case which will hit the limit of arguments for
-- prepared statements (65k).
--
-- Issue: #1702
-- distributed hypertable insert fails when # of columns are more than 65
--

-- Use default value
SET timescaledb.max_insert_batch_size TO 1000;

CREATE TABLE test_1702 (
	id varchar(100) NOT NULL,
	time timestamp NOT NULL,
	dummy1	int	,
	dummy2	int	,
	dummy4	int	,
	dummy5	int	,
	dummy6	int	,
	dummy7	int	,
	dummy8	int	,
	dummy9	int	,
	dummy10	int	,
	dummy11	int	,
	dummy12	int	,
	dummy13	int	,
	dummy14	int	,
	dummy15	int	,
	dummy16	int	,
	dummy17	int	,
	dummy18	int	,
	dummy19	int	,
	dummy20	int	,
	dummy21	int	,
	dummy22	int	,
	dummy23	int	,
	dummy24	int	,
	dummy25	int	,
	dummy26	int	,
	dummy27	int	,
	dummy28	int	,
	dummy29	int	,
	dummy30	int	,
	dummy31	int	,
	dummy32	int	,
	dummy33	int	,
	dummy34	int	,
	dummy35	int	,
	dummy36	int	,
	dummy37	int	,
	dummy38	int	,
	dummy39	int	,
	dummy40	int	,
	dummy41	int	,
	dummy42	int	,
	dummy43	int	,
	dummy44	int	,
	dummy45	int	,
	dummy46	int	,
	dummy47	int	,
	dummy48	int	,
	dummy49	int	,
	dummy50	int	,
	dummy51	int	,
	dummy52	int	,
	dummy53	int	,
	dummy54	int	,
	dummy55	int	,
	dummy56	int	,
	dummy57	int	,
	dummy58	int	,
	dummy59	int	,
	dummy60	int	,
	dummy61	int	,
	dummy62	int	,
	dummy63	int	,
	dummy64	int	,
	dummy65	int	,
	dummy66	int	,
	dummy67	int	,
	dummy68	int	,
	dummy69	int	,
	dummy70	int	,
	dummy71	int
);

SELECT create_distributed_hypertable('test_1702', 'time', 'id');

-- Original issue case
--
-- Expect batch size to be lower than defined max_insert_batch_size
--
EXPLAIN (COSTS OFF) INSERT INTO test_1702(id, time) VALUES('1', current_timestamp);
INSERT INTO test_1702(id, time) VALUES('1', current_timestamp);

EXPLAIN (COSTS OFF) INSERT INTO test_1702(id, time) SELECT generate_series(2, 1500), current_timestamp;
INSERT INTO test_1702(id, time) SELECT generate_series(2, 1500), current_timestamp;
SELECT count(*) from test_1702;

DROP TABLE test_1702;

--
-- Expect batch size to be similair to max_insert_batch_size
--
CREATE TABLE test_1702 (
	id varchar(100) NOT NULL,
	time timestamp NOT NULL,
	dummy1	int	,
	dummy2	int	,
	dummy4	int	,
	dummy5	int
	);

SELECT create_distributed_hypertable('test_1702', 'time', 'id');

EXPLAIN (COSTS OFF) INSERT INTO test_1702(id, time) VALUES('1', current_timestamp);

DROP TABLE test_1702;

--
-- Test that creating a hypertable with a space dimension and
-- if_not_exists works as expected, that is, the second call does not
-- generate an error (and does not crash).
--

CREATE TABLE whatever (
    timestamp TIMESTAMPTZ NOT NULL,
    user_id   INT         NOT NULL,
    data      JSONB
);

SELECT * FROM create_distributed_hypertable('whatever', 'timestamp', 'user_id',
                                            if_not_exists => true, chunk_time_interval => INTERVAL '1 day');
-- Check the hypertable sequence before and after call to ensure that
-- the hypertable sequence was not increased with the second call.
SELECT last_value FROM _timescaledb_catalog.hypertable_id_seq;
SELECT * FROM create_distributed_hypertable('whatever', 'timestamp', 'user_id',
                                            if_not_exists => true, chunk_time_interval => INTERVAL '1 day');
SELECT last_value FROM _timescaledb_catalog.hypertable_id_seq;

-- Test that creating a distributed hypertable from a table with data
-- fails, and that migrate_data blocked.

CREATE TABLE dist_hypertable_1 (
  time TIMESTAMPTZ NOT NULL,
  device INTEGER,
  temp FLOAT
);

INSERT INTO dist_hypertable_1 VALUES
       ('2017-01-01 06:01', 1),
       ('2017-01-01 09:11', 3),
       ('2017-01-01 08:01', 1),
       ('2017-01-02 08:01', 2),
       ('2018-07-02 08:01', 87);


\set ON_ERROR_STOP 0
SELECT * FROM create_distributed_hypertable('dist_hypertable_1', 'time', 'device', 3,
       migrate_data => FALSE);
SELECT * FROM create_distributed_hypertable('dist_hypertable_1', 'time', 'device', 3,
       migrate_data => TRUE);
\set ON_ERROR_STOP 1

-- Test creating index with transaction per chunk on a distributed hypertable
--
DROP TABLE disttable;

CREATE TABLE disttable(
    time timestamptz NOT NULL,
    device int,
    value float
);
SELECT * FROM create_distributed_hypertable('disttable', 'time', 'device', 3);
INSERT INTO disttable VALUES
       ('2017-01-01 06:01', 1, 1.2),
       ('2017-01-01 09:11', 3, 4.3),
       ('2017-01-01 08:01', 1, 7.3),
       ('2017-01-02 08:01', 2, 0.23),
       ('2018-07-02 08:01', 87, 0.0),
       ('2018-07-01 06:01', 13, 3.1),
       ('2018-07-01 09:11', 90, 10303.12),
       ('2018-07-01 08:01', 29, 64);
\set ON_ERROR_STOP 0
CREATE INDEX disttable_time_device_idx ON disttable (time, device) WITH (timescaledb.transaction_per_chunk);
\set ON_ERROR_STOP 1

-- Test using system columns with distributed hypertable
--
CREATE TABLE dist_syscol(time timestamptz NOT NULL, color int, temp float);
SELECT * FROM create_distributed_hypertable('dist_syscol', 'time', 'color');

INSERT INTO dist_syscol VALUES
	('2017-02-01 06:01', 1, 1.1),
	('2017-02-01 08:01', 1, 1.2),
	('2018-02-02 08:01', 2, 1.3),
	('2019-02-01 09:11', 3, 2.1),
	('2019-02-02 09:11', 3, 2.1),
	('2019-02-02 10:01', 5, 1.2),
	('2019-02-03 11:11', 6, 3.5),
	('2019-02-04 08:21', 4, 6.6),
	('2019-02-04 10:11', 7, 7.4),
	('2019-02-04 12:11', 8, 2.1),
	('2019-02-05 13:31', 8, 6.3),
	('2019-02-06 02:11', 5, 1.8),
	('2019-02-06 01:13', 7, 7.9),
	('2019-02-06 19:24', 9, 5.9),
	('2019-02-07 18:44', 5, 9.7),
	('2019-02-07 20:24', 6, NULL),
	('2019-02-07 09:33', 7, 9.5),
	('2019-02-08 08:54', 1, 7.3),
	('2019-02-08 18:14', 4, 8.2),
	('2019-02-09 19:23', 8, 9.1);

-- Return chunk table as a source
SET timescaledb.enable_per_data_node_queries = false;
SELECT tableoid::regclass, * FROM dist_syscol;

-- Produce an error
SET timescaledb.enable_per_data_node_queries = true;
\set ON_ERROR_STOP 0
SELECT tableoid::regclass, * FROM dist_syscol;
\set ON_ERROR_STOP 1

-----------------------
-- Test DataNodeCopy --
-----------------------
SET timescaledb.enable_distributed_insert_with_copy=true;
DROP TABLE disttable;

-- Add serial (autoincrement) and DEFAULT value columns to test that
-- these work with DataNodeCopy
CREATE TABLE disttable(
    id serial,
    time timestamptz NOT NULL,
    device int DEFAULT 100,
    temp_c float
);
SELECT * FROM create_distributed_hypertable('disttable', 'time', 'device');

-- Create a datatable to source data from. Add array of composite data
-- type to test switching to text mode below. Arrays include the type
-- Oid when serialized in binary format. Since the Oid of a
-- user-created type can differ across data nodes, such serialization
-- is not safe.
CREATE TABLE datatable (LIKE disttable);
CREATE TYPE highlow AS (high int, low int);
CALL distributed_exec($$ CREATE TYPE highlow AS (high int, low int) $$);
ALTER TABLE datatable ADD COLUMN minmaxes highlow[];
INSERT INTO datatable (id, time, device, temp_c, minmaxes) VALUES
       (1, '2017-01-01 06:01', 1, 1.2, ARRAY[(1,2)::highlow]),
       (2, '2017-01-01 09:11', 3, 4.3, ARRAY[(2,3)::highlow]),
       (3, '2017-01-01 08:01', 1, 7.3, ARRAY[(4,5)::highlow]),
       (4, '2017-01-02 08:01', 2, 0.23, ARRAY[(6,7)::highlow]),
       (5, '2018-07-02 08:01', 87, 0.0, ARRAY[(8,9)::highlow]),
       (6, '2018-07-01 06:01', 13, 3.1, ARRAY[(10,11)::highlow]),
       (7, '2018-07-01 09:11', 90, 10303.12, ARRAY[(12,13)::highlow]),
       (8, '2018-07-01 08:01', 29, 64, ARRAY[(14,15)::highlow]);

-- Show that DataNodeCopy is used instead of DataNodeDispatch. Should
-- default to FORMAT binary in the remote SQL. Add RETURNING to show
-- that it works.
EXPLAIN VERBOSE
INSERT INTO disttable (time, device, temp_c)
SELECT time, device, temp_c FROM datatable
RETURNING *;

-- Perform the actual insert
INSERT INTO disttable (time, device, temp_c)
SELECT time, device, temp_c FROM datatable
RETURNING *;

-- Show that the data was added:
SELECT * FROM disttable ORDER BY 1;
SELECT count(*) FROM disttable;

-- Add an array of a composite type to check that DataNodeCopy
-- switches to text format if we use a table with an array of a custom
-- type. There should be no "FORMAT binary" in the remote explain.
ALTER TABLE disttable ADD COLUMN minmaxes highlow[];

EXPLAIN VERBOSE
INSERT INTO disttable (time, device, temp_c, minmaxes)
SELECT time, device, temp_c, minmaxes FROM datatable;
INSERT INTO disttable (time, device, temp_c, minmaxes)
SELECT time, device, temp_c, minmaxes FROM datatable;

-- Should have double amount of rows compared to before and half of
-- them values in the new column.  Note, must use TEXT format on the
-- connection to make query work with custom type array.
SET timescaledb.enable_connection_binary_data=false;
SELECT * FROM disttable ORDER BY 1;
SELECT count(*) FROM disttable;

-- Binary format should lead to data incompatibility in PG 13 and earlier,
-- because the highlow data type has different oids on data and access nodes.
-- Use this to test the deserialization error reporting. Newer PG version
-- ignore this oid mismatch for non-builtin types.
SET timescaledb.enable_connection_binary_data=true;
\set ON_ERROR_STOP 0
SET timescaledb.remote_data_fetcher = 'copy';
SELECT * FROM disttable ORDER BY 1;

SET timescaledb.remote_data_fetcher = 'cursor';
SELECT * FROM disttable ORDER BY 1;
\set ON_ERROR_STOP 1

RESET timescaledb.remote_data_fetcher;

-- Show that DataNodeCopy is NOT used when source hypertable and target hypertable
-- of the SELECT are both distributed. Try subselects with LIMIT, RETURNING and
-- different distributed hypertable as source
EXPLAIN (COSTS OFF)
INSERT INTO disttable (time, device, temp_c)
SELECT time, device, temp_c FROM disttable;
EXPLAIN (COSTS OFF)
INSERT INTO disttable (time, device, temp_c)
SELECT time, device, temp_c FROM disttable LIMIT 1;
EXPLAIN (COSTS OFF)
INSERT INTO disttable (time, device, temp_c)
SELECT time, device, temp_c FROM disttable RETURNING *;
INSERT INTO disttable (time, device, temp_c)
SELECT time, device, temp_c FROM disttable;
INSERT INTO disttable (time, device, temp_c)
SELECT * FROM hyper_estimate LIMIT 2;
SELECT count(*) FROM disttable;

-- REMOVE a column on data nodes to check how errors are handled:
CALL distributed_exec($$ ALTER TABLE disttable DROP COLUMN minmaxes $$);

\set ON_ERROR_STOP 0
INSERT INTO disttable SELECT * FROM datatable;
\set ON_ERROR_STOP 1

DROP TABLE disttable;

-- Create a new table access method by reusing heap handler
CREATE ACCESS METHOD test_am TYPE TABLE HANDLER heap_tableam_handler;

SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$
CREATE ACCESS METHOD test_am TYPE TABLE HANDLER heap_tableam_handler;
$$);

-- Create distributed hypertable using non-default access method
CREATE TABLE disttable(time timestamptz NOT NULL, device int, temp_c float, temp_f float GENERATED ALWAYS AS (temp_c * 9 / 5 + 32) STORED) USING test_am;
SELECT * FROM create_distributed_hypertable('disttable', 'time', 'device', 3);

-- Make sure that distributed hypertable created on data nodes is
-- using the correct table access method
SELECT * FROM test.remote_exec(ARRAY[:'DATA_NODE_1', :'DATA_NODE_2', :'DATA_NODE_3'], $$

SELECT amname AS hypertable_amname
FROM pg_class cl, pg_am am
WHERE cl.oid = 'disttable'::regclass
AND cl.relam = am.oid;
$$);

-- Check that basic operations are working as expected
INSERT INTO disttable VALUES
       ('2017-01-01 06:01', 1, -10.0),
       ('2017-01-01 09:11', 3, -5.0),
       ('2017-01-01 08:01', 1, 1.0),
       ('2017-01-02 08:01', 2, 5.0),
       ('2018-07-02 08:01', 87, 10.0),
       ('2018-07-01 06:01', 13, 15.0),
       ('2018-07-01 09:11', 90, 20.0),
       ('2018-07-01 08:01', 29, 24.0);

SELECT * FROM disttable ORDER BY time;

-- Show that GENERATED columns work for INSERT with RETURNING clause
-- (should use DataNodeCopy)
TRUNCATE disttable;
EXPLAIN VERBOSE
INSERT INTO disttable VALUES ('2017-08-01 06:01', 1, 35.0) RETURNING *;
INSERT INTO disttable VALUES ('2017-08-01 06:01', 1, 35.0) RETURNING *;

-- Same values returned with SELECT:
SELECT * FROM disttable ORDER BY 1;

UPDATE disttable SET temp_c=40.0 WHERE device=1;
SELECT * FROM disttable ORDER BY 1;

-- Insert another value
INSERT INTO disttable VALUES ('2017-09-01 06:01', 2, 30.0);
SELECT * FROM disttable ORDER BY 1;
-- Delete a value based on the generated column
DELETE FROM disttable WHERE temp_f=104;

SELECT * FROM disttable ORDER BY 1;

-- Test also with DataNodeDispatch
TRUNCATE disttable;
SET timescaledb.enable_distributed_insert_with_copy=false;

EXPLAIN VERBOSE
INSERT INTO disttable VALUES ('2017-09-01 06:01', 5, 40.0) RETURNING *;
INSERT INTO disttable VALUES ('2017-09-01 06:01', 5, 40.0) RETURNING *;

-- Generated columns with SELECT
SELECT * FROM disttable ORDER BY 1;

-- Check distributed hypertable within procedure properly drops remote tables
--
-- #3663
--
CREATE TABLE test (time timestamp, v int);
SELECT create_distributed_hypertable('test','time');
CREATE PROCEDURE test_drop() LANGUAGE PLPGSQL AS $$
BEGIN
    DROP TABLE test;
END
$$;
CALL test_drop();
CREATE TABLE test (time timestamp, v int);
SELECT create_distributed_hypertable('test','time');
DROP TABLE test;


-- Test that stable functions are calculated on the access node.
--
-- As a stable function to test, use the timestamp -> timestamptz conversion
-- that is stable because it uses the current timezone.
-- We have to be careful about `timestamp < timestamptz` comparison. From postgres
-- docs:
--  When comparing a timestamp without time zone to a timestamp with time zone,
--  the former value is assumed to be given in the time zone specified by the
--  TimeZone configuration parameter, and is rotated to UTC for comparison to
--  the latter value (which is already in UTC internally).
-- We don't want this to happen on data node, so we cast the filter value to
-- timestamp, and check that this cast happens on the access node and uses the
-- current timezone.

SELECT test.tsl_override_current_timestamptz(null);
CREATE TABLE test_tz (time timestamp, v int);
SELECT create_distributed_hypertable('test_tz','time',
	chunk_time_interval => interval '1 hour');
INSERT INTO test_tz VALUES ('2018-01-02 12:00:00', 2), ('2018-01-02 11:00:00', 1),
	('2018-01-02 13:00:00', 3), ('2018-01-02 14:00:00', 4);

SET TIME ZONE 'Etc/GMT';

SELECT '2018-01-02 12:00:00 +00'::timestamptz::timestamp;

-- Normal WHERE clause on baserel
SELECT * FROM test_tz WHERE time > '2018-01-02 12:00:00 +00'::timestamptz::timestamp;

EXPLAIN (verbose, costs off)
SELECT * FROM test_tz WHERE time > '2018-01-02 12:00:00 +00'::timestamptz::timestamp;

-- Also test different code paths used with aggregation pushdown.
SELECT count(*) FROM test_tz WHERE time > '2018-01-02 12:00:00 +00'::timestamptz::timestamp;

EXPLAIN (verbose, costs off)
SELECT count(*) FROM test_tz WHERE time > '2018-01-02 12:00:00 +00'::timestamptz::timestamp;

-- TODO: test HAVING here and in the later now() tests as well.

-- Change the timezone and check that the conversion is applied correctly.
SET TIME ZONE 'Etc/GMT+1';
SELECT '2018-01-02 12:00:00 +00'::timestamptz::timestamp;

SELECT * FROM test_tz WHERE time > '2018-01-02 12:00:00 +00'::timestamptz::timestamp;

EXPLAIN (verbose, costs off)
SELECT * FROM test_tz WHERE time > '2018-01-02 12:00:00 +00'::timestamptz::timestamp;

SELECT count(*) FROM test_tz WHERE time > '2018-01-02 12:00:00 +00'::timestamptz::timestamp;

EXPLAIN (verbose, costs off)
SELECT count(*) FROM test_tz WHERE time > '2018-01-02 12:00:00 +00'::timestamptz::timestamp;

-- Conversion to timestamptz cannot be evaluated at the access node, because the
-- argument is a column reference.
SELECT count(*) FROM test_tz WHERE time::timestamptz > now();

-- According to our docs, JIT is not recommended for use on access node in
-- multi-node environment. Turn it off so that it doesn't ruin EXPLAIN for the
-- next query.
SET jit = 0;

-- Test that operators are evaluated as well. Comparison of timestamp with
-- timestamptz is a stable operator, and comparison of two timestamps is an
-- immutable operator. This also test that immutable functions using these
-- operators as arguments are evaluated.
EXPLAIN (verbose, costs off)
WITH dummy AS (SELECT '2018-01-02 12:00:00 +00'::timestamptz::timestamp x)
SELECT * FROM test_tz, dummy
WHERE time > x
	+ (x = x)::int -- stable
		* (x = '2018-01-02 11:00:00'::timestamp)::int -- immutable
		* INTERVAL '1 hour';

-- Reference value for the above test.
WITH dummy AS (SELECT '2018-01-02 12:00:00 +00'::timestamptz::timestamp x)
SELECT x + (x = x)::int * (x = '2018-01-02 11:00:00'::timestamp)::int * INTERVAL '1 hour'
FROM dummy;

-- Exercise some more stable timestamp-related functions.
EXPLAIN (COSTS OFF, VERBOSE)
SELECT * FROM test_tz WHERE date_trunc('month', time) > date_in('2021-01-01')
	AND time::time > '00:00:00'::time
		+ (INTERVAL '1 hour') * date_part('hour', INTERVAL '1 hour');


-- Check that the test function for partly overriding now() works. It's very
-- hacky and only has effect when we estimate some costs or evaluate sTABLE
-- functions in quals on access node, and has no effect in other cases.
-- Consider deleting it altogether.
SELECT test.tsl_override_current_timestamptz('2018-01-02 12:00:00 +00'::timestamptz);
SELECT count(*) FROM test_tz WHERE time > now();
SELECT test.tsl_override_current_timestamptz(null);

RESET TIME ZONE;
DROP TABLE test_tz;

-- Check that now() is evaluated on the access node. Also check that it is evaluated
-- anew on every execution of a prepared statement.
CREATE TABLE test_now (time timestamp, v int);

SELECT create_distributed_hypertable('test_now','time',
	chunk_time_interval => interval '1 hour');

PREPARE test_query as
SELECT count(*) FILTER (WHERE time = now()), count(*) FILTER (WHERE time < now()),
	count(*) FILTER (WHERE time > now()) FROM test_now;
;

BEGIN; -- to fix the value of now();

INSERT INTO test_now VALUES
	(now(), 1), (now() + INTERVAL '1 hour', 1),
	(now() + INTERVAL '2 hour', 2 ), (now() + INTERVAL '3 hour', 3);

SELECT count(*) FILTER (WHERE time = now()), count(*) FILTER (WHERE time < now()),
	count(*) FILTER (WHERE time > now()) FROM test_now;

EXECUTE test_query;

-- Also test different code paths used with aggregation pushdown.
-- We can't run EXPLAIN here, because now() is different every time. But the
-- strict equality should be enough to detect if now() is being erroneously
-- evaluated on data node, where it will differ from time to time.
SELECT count(*) FROM test_now WHERE time = now();

COMMIT;

-- now() will be different in a new transaction.
BEGIN;

SELECT count(*) FILTER (WHERE time = now()), count(*) FILTER (WHERE time < now()),
	count(*) FILTER (WHERE time > now()) FROM test_now;

EXECUTE test_query;

SELECT count(*) FROM test_now WHERE time = now();

COMMIT;

DROP TABLE test_now;

DEALLOCATE test_query;

-- Check enabling distributed compression within a
-- procedure/function works
--
-- #3705
--
CREATE TABLE test (time timestamp, v int);
SELECT create_distributed_hypertable('test','time');
CREATE PROCEDURE test_set_compression() LANGUAGE PLPGSQL AS $$
BEGIN
	ALTER TABLE test SET (timescaledb.compress);
END
$$;
CALL test_set_compression();
INSERT INTO test VALUES (now(), 0);
SELECT compress_chunk(show_chunks) FROM show_chunks('test');
DROP TABLE test;

CREATE TABLE test (time timestamp, v int);
SELECT create_distributed_hypertable('test','time');
CREATE FUNCTION test_set_compression_func() RETURNS BOOL LANGUAGE PLPGSQL AS $$
BEGIN
	ALTER TABLE test SET (timescaledb.compress);
	RETURN TRUE;
END
$$;
SELECT test_set_compression_func();
INSERT INTO test VALUES (now(), 0);
SELECT compress_chunk(show_chunks) FROM show_chunks('test');
DROP TABLE test;

-- Fix ALTER SET/DROP NULL constraint on distributed hypertable
--
-- #3860
--
CREATE TABLE test (time timestamp NOT NULL, my_column int NOT NULL);
SELECT create_distributed_hypertable('test','time');

\set ON_ERROR_STOP 0
INSERT INTO test VALUES (now(), NULL);
\set ON_ERROR_STOP 1

ALTER TABLE test ALTER COLUMN my_column DROP NOT NULL;
INSERT INTO test VALUES (now(), NULL);

\set ON_ERROR_STOP 0
ALTER TABLE test ALTER COLUMN my_column SET NOT NULL;
\set ON_ERROR_STOP 1

DELETE FROM test;
ALTER TABLE test ALTER COLUMN my_column SET NOT NULL;

DROP TABLE test;

-- Test insert into distributed hypertable with pruned chunks

CREATE TABLE pruned_chunks_1(time TIMESTAMPTZ NOT NULL, sensor_id INTEGER, value FLOAT);
SELECT table_name FROM create_distributed_hypertable('pruned_chunks_1', 'time', 'sensor_id');

INSERT INTO pruned_chunks_1 VALUES  ('2020-12-09',1,32.2);

CREATE TABLE pruned_chunks_2(time TIMESTAMPTZ NOT NULL, sensor_id INTEGER, value FLOAT);

-- Convert the table to a distributed hypertable
SELECT table_name FROM create_distributed_hypertable('pruned_chunks_2', 'time', 'sensor_id');

insert into pruned_chunks_2 select * from pruned_chunks_1;
insert into pruned_chunks_2 select * from pruned_chunks_1 WHERE time > '2022-01-01';

-- TEST freeze_chunk api. does not work for distributed chunks
SELECT chunk_schema || '.' ||  chunk_name as "CHNAME"
FROM timescaledb_information.chunks
WHERE hypertable_name = 'pruned_chunks_1'
ORDER BY chunk_name LIMIT 1
\gset

\set ON_ERROR_STOP 0
SELECT  _timescaledb_functions.freeze_chunk( :'CHNAME');
\set ON_ERROR_STOP 1

--TEST freeze_chunk api for regular hypertables. Works only for >= PG14
CREATE TABLE freeze_1(time TIMESTAMPTZ NOT NULL, sensor_id INTEGER, value FLOAT);
SELECT table_name FROM create_hypertable('freeze_1', 'time');
INSERT INTO freeze_1 VALUES  ('2020-12-09',1,32.2);
\set ON_ERROR_STOP 0
SELECT  _timescaledb_functions.freeze_chunk( ch) FROM ( select show_chunks('freeze_1') ch ) q;
\set ON_ERROR_STOP 1

DROP TABLE pruned_chunks_1;
DROP TABLE pruned_chunks_2;

-- Cleanup
DROP DATABASE :DATA_NODE_1;
DROP DATABASE :DATA_NODE_2;
DROP DATABASE :DATA_NODE_3;
